From 5b216465238db0159f10695aea16b9ffe1900046 Mon Sep 17 00:00:00 2001
From: Sohil Mehta <sohil.mehta@intel.com>
Date: Thu, 8 Sep 2022 13:32:59 -0700
Subject: [PATCH 08/28] x86/process/64: Clean up uintr task fork and exit paths

The user interrupt MSRs and the user interrupt state is task specific.
During task fork and exit clear the task state, clear the MSRs and
dereference the shared resources.

Some of the memory resources like the UPID are referenced in the file
descriptor and could be in use while the uvec_fd is still valid.
Instead of freeing up  the UPID just dereference it.  Eventually when
every user releases the reference the memory resource will be freed up.

Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
Signed-off-by: Sohil Mehta <sohil.mehta@intel.com>
---
 arch/x86/include/asm/uintr.h |  4 +++
 arch/x86/kernel/fpu/core.c   | 14 ++++++++++
 arch/x86/kernel/process.c    |  9 +++++++
 arch/x86/kernel/uintr.c      | 52 ++++++++++++++++++++++++++++++++++++
 4 files changed, 79 insertions(+)

diff --git a/arch/x86/include/asm/uintr.h b/arch/x86/include/asm/uintr.h
index 2c8156c72cc0..636e2f4e7ef4 100644
--- a/arch/x86/include/asm/uintr.h
+++ b/arch/x86/include/asm/uintr.h
@@ -34,11 +34,15 @@ struct uintr_upid_ctx {
 void switch_uintr_prepare(struct task_struct *prev);
 void switch_uintr_return(void);
 
+void uintr_free(struct task_struct *task);
+
 #else /* !CONFIG_X86_USER_INTERRUPTS */
 
 static inline void switch_uintr_prepare(struct task_struct *prev) {}
 static inline void switch_uintr_return(void) {}
 
+static inline void uintr_free(struct task_struct *task) {}
+
 #endif /* CONFIG_X86_USER_INTERRUPTS */
 
 #endif /* _ASM_X86_UINTR_H */
diff --git a/arch/x86/kernel/fpu/core.c b/arch/x86/kernel/fpu/core.c
index 8a3197adbb4b..c640526c62e9 100644
--- a/arch/x86/kernel/fpu/core.c
+++ b/arch/x86/kernel/fpu/core.c
@@ -677,6 +677,7 @@ int fpu_clone(struct task_struct *dst, u64 clone_flags, bool minimal,
 	 * thus x86_task_fpu() will always be cacheline aligned as well.
 	 */
 	struct fpu *dst_fpu = (void *)dst + sizeof(*dst);
+	struct uintr_state *uintr_state;
 
 	BUILD_BUG_ON(sizeof(*dst) % SMP_CACHE_BYTES != 0);
 
@@ -728,6 +729,19 @@ int fpu_clone(struct task_struct *dst, u64 clone_flags, bool minimal,
 	if (!(clone_flags & CLONE_THREAD))
 		fpu_inherit_perms(dst_fpu);
 
+	/*
+	 * All of UINTR state is not expected to be inherited. The UPID related
+	 * structs are task specific. The UITT is same across a clone() but it
+	 * would be fixed up upon first execution of SENDUIPI.
+	 *
+	 * Check if the xsave header needs to be set to init value (like PASID)
+	 */
+	if (cpu_feature_enabled(X86_FEATURE_UINTR)) {
+		uintr_state = get_xsave_addr(&dst_fpu->fpstate->regs.xsave, XFEATURE_UINTR);
+		if (uintr_state)
+			memset(uintr_state, 0, sizeof(*uintr_state));
+	}
+
 	/*
 	 * Children never inherit PASID state.
 	 * Force it to have its init value:
diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c
index 4c718f8adc59..b5e62abbf938 100644
--- a/arch/x86/kernel/process.c
+++ b/arch/x86/kernel/process.c
@@ -29,6 +29,7 @@
 #include <trace/events/power.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/entry-common.h>
+#include <asm/uintr.h>
 #include <asm/cpu.h>
 #include <asm/cpuid/api.h>
 #include <asm/apic.h>
@@ -111,6 +112,13 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	dst->thread.vm86 = NULL;
 #endif
 
+#ifdef CONFIG_X86_USER_INTERRUPTS
+	/* User Interrupt receiver upid state is unique for each task */
+	dst->thread.upid_ctx = NULL;
+
+	dst->thread.upid_activated = false;
+#endif
+
 	return 0;
 }
 
@@ -135,6 +143,7 @@ void exit_thread(struct task_struct *tsk)
 	free_vm86(t);
 
 	shstk_free(tsk);
+	uintr_free(tsk);
 	fpu__drop(tsk);
 }
 
diff --git a/arch/x86/kernel/uintr.c b/arch/x86/kernel/uintr.c
index 83bfdf338fce..dfba09a97a01 100644
--- a/arch/x86/kernel/uintr.c
+++ b/arch/x86/kernel/uintr.c
@@ -351,3 +351,55 @@ void switch_uintr_return(void)
 	if (READ_ONCE(upid->puir))
 		apic->send_IPI_self(UINTR_NOTIFICATION_VECTOR);
 }
+
+/*
+ * This should only be called from exit_thread().
+ * exit_thread() can happen in current context when the current thread is
+ * exiting or it can happen for a new thread that is being created.
+ * For new threads is_uintr_task() will fail.
+ */
+void uintr_free(struct task_struct *t)
+{
+	struct uintr_upid_ctx *upid_ctx;
+
+	if (!cpu_feature_enabled(X86_FEATURE_UINTR))
+		return;
+
+	upid_ctx = t->thread.upid_ctx;
+	if (is_uintr_receiver(t)) {
+		fpregs_lock_and_load();
+
+		wrmsrl(MSR_IA32_UINTR_MISC, 0);
+		wrmsrl(MSR_IA32_UINTR_TT, 0);
+		wrmsrl(MSR_IA32_UINTR_PD, 0);
+		wrmsrl(MSR_IA32_UINTR_RR, 0);
+		wrmsrl(MSR_IA32_UINTR_STACKADJUST, 0);
+		wrmsrl(MSR_IA32_UINTR_HANDLER, 0);
+
+		/* If upid is active, upid_ctx will be valid */
+		if (is_uintr_receiver(t)) {
+			/*
+			 * Suppress notifications so that no further interrupts are
+			 * generated based on this UPID.
+			 */
+			set_bit(UINTR_UPID_STATUS_SN, (unsigned long *)&upid_ctx->upid->nc.status);
+			put_upid_ref(upid_ctx);
+		}
+
+		t->thread.upid_activated = false;
+
+		fpregs_lock();
+	}
+
+	if (upid_ctx) {
+		put_upid_ref(t->thread.upid_ctx);
+		/*
+		 * This might not be needed since the thread is exiting. Have
+		 * it anyways to be safe.
+		 */
+		t->thread.upid_ctx = NULL;
+	}
+
+	//if (WARN_ON_ONCE(t != current))
+	//	return;
+}
-- 
2.52.0

