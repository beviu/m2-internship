From 237dca172f6d1dc8b3a14b580dddee7581ab87c8 Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Tue, 13 May 2025 11:27:59 +0200
Subject: [PATCH 4/5] mm: add fast_tracepoints to PF handling

---
 arch/alpha/mm/fault.c         |  2 +-
 arch/arc/mm/fault.c           |  2 +-
 arch/arm/mm/fault.c           |  2 +-
 arch/arm64/mm/fault.c         |  2 +-
 arch/csky/mm/fault.c          |  2 +-
 arch/hexagon/mm/vm_fault.c    |  2 +-
 arch/ia64/mm/fault.c          |  2 +-
 arch/loongarch/mm/fault.c     |  2 +-
 arch/m68k/mm/fault.c          |  2 +-
 arch/microblaze/mm/fault.c    |  2 +-
 arch/nios2/mm/fault.c         |  2 +-
 arch/openrisc/mm/fault.c      |  2 +-
 arch/parisc/mm/fault.c        |  2 +-
 arch/powerpc/mm/copro_fault.c |  3 +-
 arch/powerpc/mm/fault.c       |  2 +-
 arch/riscv/mm/fault.c         |  2 +-
 arch/s390/mm/fault.c          |  2 +-
 arch/sh/mm/fault.c            |  2 +-
 arch/sparc/mm/fault_32.c      |  4 +--
 arch/sparc/mm/fault_64.c      |  2 +-
 arch/um/kernel/trap.c         |  2 +-
 arch/x86/mm/fault.c           | 33 +++++++++++++++++----
 arch/xtensa/mm/fault.c        |  2 +-
 drivers/iommu/amd/iommu_v2.c  |  2 +-
 drivers/iommu/iommu-sva.c     |  2 +-
 fs/userfaultfd.c              |  7 ++++-
 include/linux/mm.h            |  2 +-
 include/linux/userfaultfd_k.h |  2 +-
 include/trace/fast.h          | 18 ++++++++++--
 mm/gup.c                      |  4 +--
 mm/hmm.c                      |  2 +-
 mm/huge_memory.c              |  4 +--
 mm/hugetlb.c                  |  4 +--
 mm/ksm.c                      |  2 +-
 mm/memory.c                   | 54 ++++++++++++++++++++++++-----------
 mm/shmem.c                    |  4 +--
 36 files changed, 126 insertions(+), 61 deletions(-)

diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
index ef427a6bdd1a..14972abf2242 100644
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@ -150,7 +150,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
 	/* If for any reason at all we couldn't handle the fault,
 	   make sure we exit gracefully rather than endlessly redo
 	   the fault.  */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c
index 5ca59a482632..69c1b9a3d43b 100644
--- a/arch/arc/mm/fault.c
+++ b/arch/arc/mm/fault.c
@@ -137,7 +137,7 @@ void do_page_fault(unsigned long address, struct pt_regs *regs)
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	/* Quick path to respond to signals */
 	if (fault_signal_pending(fault, regs)) {
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index 2418f1efabd8..b498240bbaeb 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -256,7 +256,7 @@ __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int flags,
 	if (!(vma->vm_flags & vma_flags))
 		return VM_FAULT_BADACCESS;
 
-	return handle_mm_fault(vma, addr & PAGE_MASK, flags, regs);
+	return handle_mm_fault(vma, addr & PAGE_MASK, flags, regs, false);
 }
 
 static int __kprobes
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 596f46dabe4e..9144ebb28859 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -509,7 +509,7 @@ static vm_fault_t __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	 */
 	if (!(vma->vm_flags & vm_flags))
 		return VM_FAULT_BADACCESS;
-	return handle_mm_fault(vma, addr, mm_flags, regs);
+	return handle_mm_fault(vma, addr, mm_flags, regs, false);
 }
 
 static bool is_el0_instruction_abort(unsigned long esr)
diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index e15f736cca4b..d48d1b5584e1 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -272,7 +272,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c
index f73c7cbfe326..a9a9353fe1ef 100644
--- a/arch/hexagon/mm/vm_fault.c
+++ b/arch/hexagon/mm/vm_fault.c
@@ -91,7 +91,7 @@ void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)
 		break;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c
index ef78c2d66cdd..47d57d795e9d 100644
--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@ -134,7 +134,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re
 	 * sure we exit gracefully rather than endlessly redo the
 	 * fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/loongarch/mm/fault.c b/arch/loongarch/mm/fault.c
index 1ccd53655cab..f5fa6dc420f7 100644
--- a/arch/loongarch/mm/fault.c
+++ b/arch/loongarch/mm/fault.c
@@ -208,7 +208,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
index 4d2837eb3e2a..873fbdac33c1 100644
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@ -135,7 +135,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	pr_debug("handle_mm_fault returns %x\n", fault);
 
 	if (fault_signal_pending(fault, regs))
diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
index 5c40c3ebe52f..2c331615f213 100644
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@ -217,7 +217,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c
index edaca0a6c1c1..ac253fc3de49 100644
--- a/arch/nios2/mm/fault.c
+++ b/arch/nios2/mm/fault.c
@@ -134,7 +134,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c
index b4762d66e9ef..3aa71cdc3d4c 100644
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@ -160,7 +160,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c
index 869204e97ec9..0e0c65eb6552 100644
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@ -306,7 +306,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
 	 * fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c
index 7c507fb48182..8ab4c75273e5 100644
--- a/arch/powerpc/mm/copro_fault.c
+++ b/arch/powerpc/mm/copro_fault.c
@@ -64,7 +64,8 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,
 	}
 
 	ret = 0;
-	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL);
+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL,
+			       false);
 
 	/* The fault is fully completed (including releasing mmap lock) */
 	if (*flt & VM_FAULT_COMPLETED)
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 2bef19cc1b98..994138e5ee06 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -523,7 +523,7 @@ static int ___do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	major |= fault & VM_FAULT_MAJOR;
 
diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c
index d86f7cebd4a7..5aff25f999ad 100644
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@ -317,7 +317,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index 9649d9382e0a..e8616086d021 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -425,7 +425,7 @@ static inline vm_fault_t do_exception(struct pt_regs *regs, int access)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	if (fault_signal_pending(fault, regs)) {
 		fault = VM_FAULT_SIGNAL;
 		if (flags & FAULT_FLAG_RETRY_NOWAIT)
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index acd2f5e50bfc..848a95043e15 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -479,7 +479,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (unlikely(fault & (VM_FAULT_RETRY | VM_FAULT_ERROR)))
 		if (mm_fault_error(regs, error_code, address, fault))
diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c
index 91259f291c54..e852be5e1c18 100644
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@ -185,7 +185,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
@@ -338,7 +338,7 @@ static void force_user_fault(unsigned long address, int write)
 		if (!(vma->vm_flags & (VM_READ | VM_EXEC)))
 			goto bad_area;
 	}
-	switch (handle_mm_fault(vma, address, flags, NULL)) {
+	switch (handle_mm_fault(vma, address, flags, NULL, false)) {
 	case VM_FAULT_SIGBUS:
 	case VM_FAULT_OOM:
 		goto do_sigbus;
diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c
index 4acc12eafbf5..834b70a5dbfc 100644
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@ -422,7 +422,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
 			goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		goto exit_exception;
diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c
index d3ce21c4ca32..d8a1b01459ce 100644
--- a/arch/um/kernel/trap.c
+++ b/arch/um/kernel/trap.c
@@ -71,7 +71,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,
 	do {
 		vm_fault_t fault;
 
-		fault = handle_mm_fault(vma, address, flags, NULL);
+		fault = handle_mm_fault(vma, address, flags, NULL, false);
 
 		if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 			goto out_nosemaphore;
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 01f93d026429..eddb7739b865 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1257,7 +1257,8 @@ NOKPROBE_SYMBOL(do_kern_addr_fault);
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
 			unsigned long error_code,
-			unsigned long address)
+			unsigned long address,
+			bool trace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
@@ -1373,6 +1374,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * 1. Failed to acquire mmap_lock, and
 	 * 2. The access did not originate in userspace.
 	 */
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(first_try_lock_mm_and_find_vma_start);
 	if (unlikely(!mmap_read_trylock(mm))) {
 		if (!user_mode(regs) && !search_exception_tables(regs->ip)) {
 			/*
@@ -1383,6 +1386,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 			return;
 		}
 retry:
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace) && (flags & FAULT_FLAG_TRIED))
+			fast_tracepoint(retry_lock_mm_and_find_vma_start);
 		mmap_read_lock(mm);
 	} else {
 		/*
@@ -1394,6 +1399,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 	}
 
 	vma = find_vma(mm, address);
+	if (IS_ENABLED(CONFIG_TRACE_PF)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_lock_mm_and_find_vma_start);
+		else
+			fast_tracepoint(first_try_lock_mm_and_find_vma_start);
+	}
 	if (unlikely(!vma)) {
 		bad_area(regs, error_code, address);
 		return;
@@ -1440,7 +1451,19 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_start);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_start);
+	}
+	fault = handle_mm_fault(vma, address, flags, regs, trace);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_end);
+	}
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
@@ -1520,7 +1543,7 @@ trace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,
 
 static __always_inline void
 handle_page_fault(struct pt_regs *regs, unsigned long error_code,
-			      unsigned long address)
+			      unsigned long address, bool trace)
 {
 	trace_page_fault_entries(regs, error_code, address);
 
@@ -1531,7 +1554,7 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, error_code, address);
 	} else {
-		do_user_addr_fault(regs, error_code, address);
+		do_user_addr_fault(regs, error_code, address, trace);
 		/*
 		 * User address page fault handling might have reenabled
 		 * interrupts. Fixing up all potential exit points of
@@ -1591,7 +1614,7 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	state = irqentry_enter(regs);
 
 	instrumentation_begin();
-	handle_page_fault(regs, error_code, address);
+	handle_page_fault(regs, error_code, address, trace);
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
index faf7cf35a0ee..c3aaf804470c 100644
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@ -164,7 +164,7 @@ void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/drivers/iommu/amd/iommu_v2.c b/drivers/iommu/amd/iommu_v2.c
index 864e4ffb6aa9..7b0f28fe775c 100644
--- a/drivers/iommu/amd/iommu_v2.c
+++ b/drivers/iommu/amd/iommu_v2.c
@@ -494,7 +494,7 @@ static void do_fault(struct work_struct *work)
 	if (access_error(vma, fault))
 		goto out;
 
-	ret = handle_mm_fault(vma, address, flags, NULL);
+	ret = handle_mm_fault(vma, address, flags, NULL, false);
 out:
 	mmap_read_unlock(mm);
 
diff --git a/drivers/iommu/iommu-sva.c b/drivers/iommu/iommu-sva.c
index 24bf9b2b58aa..2704a677d620 100644
--- a/drivers/iommu/iommu-sva.c
+++ b/drivers/iommu/iommu-sva.c
@@ -228,7 +228,7 @@ iommu_sva_handle_iopf(struct iommu_fault *fault, void *data)
 		/* Access fault */
 		goto out_put_mm;
 
-	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL);
+	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL, false);
 	status = ret & VM_FAULT_ERROR ? IOMMU_PAGE_RESP_INVALID :
 		IOMMU_PAGE_RESP_SUCCESS;
 
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index cc694846617a..f9f84933b220 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -31,6 +31,7 @@
 #include <linux/hugetlb.h>
 #include <linux/swapops.h>
 #include <linux/miscdevice.h>
+#include <trace/fast.h>
 
 int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -389,7 +390,7 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace)
 {
 	struct mm_struct *mm = vmf->vma->vm_mm;
 	struct userfaultfd_ctx *ctx;
@@ -532,7 +533,11 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	mmap_read_unlock(mm);
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_start);
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_end);
 		schedule();
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index f13f20258ce9..74a30aea7318 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2029,7 +2029,7 @@ int generic_error_remove_page(struct address_space *mapping, struct page *page);
 #ifdef CONFIG_MMU
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
 				  unsigned long address, unsigned int flags,
-				  struct pt_regs *regs);
+				  struct pt_regs *regs, bool trace);
 extern int fixup_user_fault(struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 9df0b9a762cc..d3017846fb94 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -38,7 +38,7 @@
 
 extern int sysctl_unprivileged_userfaultfd;
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace);
 
 /*
  * The mode of operation for __mcopy_atomic and its helpers.
diff --git a/include/trace/fast.h b/include/trace/fast.h
index 77730c143325..b5ad1f8cb29a 100644
--- a/include/trace/fast.h
+++ b/include/trace/fast.h
@@ -9,8 +9,22 @@
 
 #include <asm/msr.h>
 
-#define ENUMERATE_FAST_TRACEPOINTS(x) \
-	x(c_entry)		      \
+#define ENUMERATE_FAST_TRACEPOINTS(x)				      \
+	x(c_entry)						      \
+	x(first_try_lock_mm_and_find_vma_start)			      \
+	x(first_try_lock_mm_and_find_vma_end)			      \
+	x(first_try_handle_mm_fault_with_mm_lock_start)		      \
+	x(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end) \
+	x(page_allocation_start)				      \
+	x(page_allocation_end)					      \
+	x(wake_up_userfaultfd_start)				      \
+	x(wake_up_userfaultfd_end)				      \
+	x(first_try_handle_mm_fault_with_mm_lock_end)		      \
+	x(retry_lock_mm_and_find_vma_start)			      \
+	x(retry_lock_mm_and_find_vma_end)			      \
+	x(retry_handle_mm_fault_with_mm_lock_start)		      \
+	x(retry_handle_mm_fault_with_mm_lock_page_table_walk_end)     \
+	x(retry_handle_mm_fault_with_mm_lock_end)		      \
 	x(c_exit)
 
 #define DECLARE_FAST_TRACEPOINT(name) extern u64 fast_tracepoint_##name;
diff --git a/mm/gup.c b/mm/gup.c
index e16ba75a21dc..1cf4e47cf7f4 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -928,7 +928,7 @@ static int faultin_page(struct vm_area_struct *vma,
 		VM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
@@ -1287,7 +1287,7 @@ int fixup_user_fault(struct mm_struct *mm,
 	    fatal_signal_pending(current))
 		return -EINTR;
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
diff --git a/mm/hmm.c b/mm/hmm.c
index 601a99ce3c84..6028b2b97012 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -77,7 +77,7 @@ static int hmm_vma_fault(unsigned long addr, unsigned long end,
 	}
 
 	for (; addr < end; addr += PAGE_SIZE)
-		if (handle_mm_fault(vma, addr, fault_flags, NULL) &
+		if (handle_mm_fault(vma, addr, fault_flags, NULL, false) &
 		    VM_FAULT_ERROR)
 			return -EFAULT;
 	return -EBUSY;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 0f4666757262..5e06662bb348 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -699,7 +699,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 			spin_unlock(vmf->ptl);
 			put_page(page);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -830,7 +830,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_page(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 2552f5289a1b..6124741777fb 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5735,7 +5735,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_area_struct *vma,
 	hugetlb_vma_unlock_read(vma);
 	hash = hugetlb_fault_mutex_hash(mapping, idx);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-	return handle_userfault(&vmf, reason);
+	return handle_userfault(&vmf, reason, false);
 }
 
 /*
@@ -6188,7 +6188,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		}
 		hugetlb_vma_unlock_read(vma);
 		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-		return handle_userfault(&vmf, VM_UFFD_WP);
+		return handle_userfault(&vmf, VM_UFFD_WP, false);
 	}
 
 	/*
diff --git a/mm/ksm.c b/mm/ksm.c
index addf490da146..f960e40a9e1a 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -482,7 +482,7 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 			return 0;
 		ret = handle_mm_fault(vma, addr,
 				      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,
-				      NULL);
+				      NULL, false);
 	} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop until we no longer find a KSM page because
diff --git a/mm/memory.c b/mm/memory.c
index f72d70c7b2e9..4d25eb3483a5 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -79,6 +79,7 @@
 #include <linux/sched/sysctl.h>
 
 #include <trace/events/kmem.h>
+#include <trace/fast.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -86,6 +87,7 @@
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
+#include <asm/msr.h>
 
 #include "pgalloc-track.h"
 #include "internal.h"
@@ -3070,7 +3072,7 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
  *   held to the old page, as well as updating the rmap.
  * - In any case, unlock the PTL and drop the reference we took to the old page.
  */
-static vm_fault_t wp_page_copy(struct vm_fault *vmf)
+static vm_fault_t wp_page_copy(struct vm_fault *vmf, bool trace)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
 	struct vm_area_struct *vma = vmf->vma;
@@ -3088,13 +3090,21 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		goto oom;
 
 	if (is_zero_pfn(pte_pfn(vmf->orig_pte))) {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_start);
 		new_page = alloc_zeroed_user_highpage_movable(vma,
 							      vmf->address);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_end);
 		if (!new_page)
 			goto oom;
 	} else {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_start);
 		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,
 				vmf->address);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_end);
 		if (!new_page)
 			goto oom;
 
@@ -3349,7 +3359,7 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf)
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_wp_page(struct vm_fault *vmf)
+static vm_fault_t do_wp_page(struct vm_fault *vmf, bool trace)
 	__releases(vmf->ptl)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
@@ -3359,7 +3369,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (likely(!unshare)) {
 		if (userfaultfd_pte_wp(vma, *vmf->pte)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, trace);
 		}
 
 		/*
@@ -3457,7 +3467,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (folio && folio_test_ksm(folio))
 		count_vm_event(COW_KSM);
 #endif
-	return wp_page_copy(vmf);
+	return wp_page_copy(vmf, trace);
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
@@ -4011,7 +4021,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		ret |= do_wp_page(vmf);
+		ret |= do_wp_page(vmf, false);
 		if (ret & VM_FAULT_ERROR)
 			ret &= VM_FAULT_ERROR;
 		goto out;
@@ -4045,7 +4055,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct page *page;
@@ -4090,7 +4100,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 		}
 		goto setpte;
 	}
@@ -4098,7 +4108,11 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	/* Allocate our own private page. */
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_start);
 	page = alloc_zeroed_user_highpage_movable(vma, vmf->address);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_end);
 	if (!page)
 		goto oom;
 
@@ -4133,7 +4147,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		put_page(page);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 	}
 
 	inc_mm_counter(vma->vm_mm, MM_ANONPAGES);
@@ -4842,7 +4856,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 	if (vma_is_anonymous(vmf->vma)) {
 		if (likely(!unshare) &&
 		    userfaultfd_huge_pmd_wp(vmf->vma, vmf->orig_pmd))
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, false);
 		return do_huge_pmd_wp_page(vmf);
 	}
 
@@ -4999,7 +5013,7 @@ static int do_fake_page_fault(struct mm_struct *mm, struct vm_area_struct *vma,
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, bool trace)
 {
 	pte_t entry;
 	struct vm_area_struct *vma = vmf->vma;
@@ -5055,9 +5069,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		}
 	}
 
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (vmf->flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_page_table_walk_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end);
+	}
+
 	if (!vmf->pte) {
 		if (vma_is_anonymous(vmf->vma))
-			return do_anonymous_page(vmf);
+			return do_anonymous_page(vmf, trace);
 		else
 			return do_fault(vmf);
 	}
@@ -5116,7 +5137,7 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 	if (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {
 		if (!pte_write(entry))
-			return do_wp_page(vmf);
+			return do_wp_page(vmf, trace);
 		else if (likely(vmf->flags & FAULT_FLAG_WRITE))
 			entry = pte_mkdirty(entry);
 	}
@@ -5192,7 +5213,8 @@ static int transparent_fake_fault(struct mm_struct *mm, struct vm_area_struct *v
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		bool trace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -5311,7 +5333,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, trace);
 }
 
 /**
@@ -5427,7 +5449,7 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-			   unsigned int flags, struct pt_regs *regs)
+			   unsigned int flags, struct pt_regs *regs, bool trace)
 {
 	vm_fault_t ret;
 
@@ -5457,7 +5479,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, trace);
 
 	lru_gen_exit_fault();
 
diff --git a/mm/shmem.c b/mm/shmem.c
index 0005ab2c29af..5d2ebd0bac57 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1864,7 +1864,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 			folio_unlock(folio);
 			folio_put(folio);
 		}
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, false);
 		return 0;
 	}
 
@@ -1905,7 +1905,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, false);
 		return 0;
 	}
 
-- 
2.49.0

