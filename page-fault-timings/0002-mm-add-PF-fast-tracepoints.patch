From c48c10443e4a7d184e72071182697059402750ad Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Fri, 2 May 2025 14:35:23 +0200
Subject: [PATCH 2/3] mm: add PF fast tracepoints

The code activates if ebx contains the value 0xb141a52a before a page
fault.

It collect CPU timestamp counters (TSC) at various points during page
fault handling that are accessible through sysctls.
---
 arch/x86/mm/fault.c           |  50 ++++++++++++----
 drivers/iommu/iommu-sva.c     |   2 +-
 fs/userfaultfd.c              |   8 ++-
 include/linux/mm.h            |   2 +-
 include/linux/userfaultfd_k.h |   2 +-
 include/trace/fast.h          | 105 ++++++++++++++++++++++++++++++++++
 kernel/trace/Kconfig          |   9 +++
 kernel/trace/Makefile         |   1 +
 kernel/trace/fast.c           |  41 +++++++++++++
 mm/gup.c                      |   4 +-
 mm/huge_memory.c              |   4 +-
 mm/hugetlb.c                  |   4 +-
 mm/ksm.c                      |   2 +-
 mm/memory.c                   |  40 ++++++++-----
 mm/shmem.c                    |   4 +-
 15 files changed, 240 insertions(+), 38 deletions(-)
 create mode 100644 include/trace/fast.h
 create mode 100644 kernel/trace/fast.c

diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 5651689f4761..2ade5c863069 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -36,10 +36,13 @@
 #include <asm/irq_stack.h>
 #include <asm/fred.h>
 #include <asm/sev.h>			/* snp_dump_hva_rmpentry()	*/
+#include <asm/msr.h>
 
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
+#include <trace/fast.h>
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1209,7 +1212,8 @@ NOKPROBE_SYMBOL(do_kern_addr_fault);
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
 			unsigned long error_code,
-			unsigned long address)
+			unsigned long address,
+			bool trace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
@@ -1325,16 +1329,26 @@ void do_user_addr_fault(struct pt_regs *regs,
 	if (!(flags & FAULT_FLAG_USER))
 		goto lock_mmap;
 
+	if (likely(trace))
+		fast_tracepoint(lock_vma_under_rcu_start);
 	vma = lock_vma_under_rcu(mm, address);
 	if (!vma)
 		goto lock_mmap;
+	if (likely(trace))
+		fast_tracepoint(lock_vma_under_rcu_end);
 
 	if (unlikely(access_error(error_code, vma))) {
 		bad_area_access_error(regs, error_code, address, NULL, vma);
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
 		return;
 	}
-	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);
+
+	if (likely(trace))
+		fast_tracepoint(first_handle_mm_fault_start);
+	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs, trace);
+	if (likely(trace))
+		fast_tracepoint(first_handle_mm_fault_end);
+
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -1354,10 +1368,16 @@ void do_user_addr_fault(struct pt_regs *regs,
 						 ARCH_DEFAULT_PKEY);
 		return;
 	}
+
 lock_mmap:
 
 retry:
+	if (likely(trace))
+		fast_tracepoint(lock_mm_and_find_vma_start);
 	vma = lock_mm_and_find_vma(mm, address, regs);
+	if (likely(trace))
+		fast_tracepoint(lock_mm_and_find_vma_end);
+
 	if (unlikely(!vma)) {
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
@@ -1385,7 +1405,11 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	if (likely(trace))
+		fast_tracepoint(second_handle_mm_fault_start);
+	fault = handle_mm_fault(vma, address, flags, regs, trace);
+	if (likely(trace))
+		fast_tracepoint(second_handle_mm_fault_end);
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
@@ -1466,7 +1490,7 @@ trace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,
 
 static __always_inline void
 handle_page_fault(struct pt_regs *regs, unsigned long error_code,
-			      unsigned long address)
+			      unsigned long address, bool trace)
 {
 	trace_page_fault_entries(regs, error_code, address);
 
@@ -1477,7 +1501,7 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, error_code, address);
 	} else {
-		do_user_addr_fault(regs, error_code, address);
+		do_user_addr_fault(regs, error_code, address, trace);
 		/*
 		 * User address page fault handling might have reenabled
 		 * interrupts. Fixing up all potential exit points of
@@ -1494,12 +1518,12 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	irqentry_state_t state;
 	unsigned long address;
 	bool trace = (regs->bx & 0xffffffff) == 0xb141a52a;
+	u64 timestamp;
 
 	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
-		/* Save the PF ISR entry timestamp because it will be overwritten in the
-		   assembly return code. */
-		regs->di = regs->ax & 0xffffffff;
-		regs->si = regs->dx & 0xffffffff;
+		timestamp = (regs->ax & 0xffffffff) | ((regs->dx & 0xffffffff) << 32);
+		fast_tracepoints_begin(isr_entry, timestamp);
+		fast_tracepoint(c_entry);
 	}
 
 	address = cpu_feature_enabled(X86_FEATURE_FRED) ? fred_event_data(regs) : read_cr2();
@@ -1543,8 +1567,14 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	state = irqentry_enter(regs);
 
 	instrumentation_begin();
-	handle_page_fault(regs, error_code, address);
+	handle_page_fault(regs, error_code, address, trace);
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
+
+	if (likely(trace)) {
+		timestamp = fast_tracepoints_end(c_exit);
+		regs->di = timestamp & 0xffffffff;
+		regs->si = timestamp >> 32;
+	}
 }
diff --git a/drivers/iommu/iommu-sva.c b/drivers/iommu/iommu-sva.c
index ab18bc494eef..ec58dff400ee 100644
--- a/drivers/iommu/iommu-sva.c
+++ b/drivers/iommu/iommu-sva.c
@@ -248,7 +248,7 @@ iommu_sva_handle_mm(struct iommu_fault *fault, struct mm_struct *mm)
 		/* Access fault */
 		goto out_put_mm;
 
-	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL);
+	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL, false);
 	status = ret & VM_FAULT_ERROR ? IOMMU_PAGE_RESP_INVALID :
 		IOMMU_PAGE_RESP_SUCCESS;
 
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index d80f94346199..b74ca28a5989 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -32,6 +32,8 @@
 #include <linux/swapops.h>
 #include <linux/miscdevice.h>
 #include <linux/uio.h>
+#include <asm/msr.h>
+#include <trace/fast.h>
 
 static int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -360,7 +362,7 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *mm = vma->vm_mm;
@@ -500,7 +502,11 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	release_fault_lock(vmf);
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
+		if (likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_start);
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		if (likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_end);
 		schedule();
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index bf55206935c4..fbcb5ebaf2ec 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2539,7 +2539,7 @@ struct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,
 #ifdef CONFIG_MMU
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
 				  unsigned long address, unsigned int flags,
-				  struct pt_regs *regs);
+				  struct pt_regs *regs, bool trace);
 extern int fixup_user_fault(struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 75342022d144..980b1beffc78 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -82,7 +82,7 @@ struct userfaultfd_ctx {
 	struct mm_struct *mm;
 };
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace);
 
 /* A combined operation mode + behavior flags. */
 typedef unsigned int __bitwise uffd_flags_t;
diff --git a/include/trace/fast.h b/include/trace/fast.h
new file mode 100644
index 000000000000..cfb70fb4394c
--- /dev/null
+++ b/include/trace/fast.h
@@ -0,0 +1,105 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _TRACE_FAST_H
+#define _TRACE_FAST_H
+
+#include <linux/compiler.h>
+#include <linux/printk.h>
+#include <linux/types.h>
+
+#include <asm/msr.h>
+
+#ifdef CONFIG_FAST_TRACEPOINTS
+
+#ifdef CONFIG_FAST_TRACEPOINTS_PRINTK
+
+extern const char *prev_fast_tracepoint_name;
+
+extern u64 prev_fast_tracepoint_timestamp;
+
+static inline void __fast_tracepoints_begin(const char *first_tracepoint_name,
+                                            u64 start_timestamp)
+{
+	prev_fast_tracepoint_name = first_fast_tracepoint;
+	prev_fast_tracepoint_timestamp = start_timestamp;
+}
+
+#define fast_tracepoints_begin(first_tracepoint_name, start_timestamp) \
+	__fast_tracepoints_begin(#first_tracepoint_name, #start_timestamp)
+
+static inline void __fast_tracepoint_common(const char *tracepoint_name)
+{
+	u64 now = rdtsc_ordered();
+
+	printk("fast_tracepoints: %s (%llu) to %s (%llu)\n",
+	       prev_fast_tracepoint_name, prev_fast_tracepoint_timestamp,
+	       tracepoint_name, now);
+}
+
+static inline void __fast_tracepoint(const char *tracepoint_name)
+{
+	__fast_tracepoint_common(tracepoint_name);
+
+	prev_fast_tracepoint_name = tracepoint_name;
+	prev_fast_tracepoint_timestamp = rdtsc_ordered();
+}
+
+#define fast_tracepoint(tracepoint_name) __fast_tracepoint(#tracepoint_name)
+
+static inline u64 __fast_tracepoints_end(const char *tracepoint_name)
+{
+	__fast_tracepoint_common(tracepoint_name);
+
+	return rdtsc_ordered();
+}
+
+#define fast_tracepoints_end(tracepoint_name) \
+	__fast_tracepoints_end(#tracepoint_name)
+
+#else
+
+#define ENUMERATE_FAST_TRACEPOINTS(x) \
+	x(isr_entry) \
+	x(c_entry) \
+	x(lock_vma_under_rcu_start) \
+	x(lock_vma_under_rcu_end) \
+	x(first_handle_mm_fault_start) \
+	x(first_page_table_walk_end) \
+	x(wake_up_userfaultfd_start) \
+	x(wake_up_userfaultfd_end) \
+	x(first_handle_mm_fault_end) \
+	x(lock_mm_and_find_vma_start) \
+	x(lock_mm_and_find_vma_end) \
+	x(second_handle_mm_fault_start) \
+	x(second_page_table_walk_end) \
+	x(second_handle_mm_fault_end)
+
+#define DECLARE_FAST_TRACEPOINT(name) extern u64 fast_tracepoint_timestamp_##name;
+ENUMERATE_FAST_TRACEPOINTS(DECLARE_FAST_TRACEPOINT)
+#undef DECLARE_FAST_TRACEPOINT
+
+#define fast_tracepoints_begin(tracepoint_name, start_timestamp)               \
+	do {                                                                   \
+		fast_tracepoint_timestamp_##tracepoint_name = start_timestamp; \
+	} while (0)
+
+#define fast_tracepoint(tracepoint_name) \
+	fast_tracepoint_timestamp_##tracepoint_name = rdtsc_ordered()
+
+static inline u64 __fast_tracepoints_end(void)
+{
+	return rdtsc_ordered();
+}
+
+#define fast_tracepoints_end(tracepoint_name) __fast_tracepoints_end()
+
+#endif /* CONFIG_FAST_TRACEPOINTS_PRINTK */
+
+#else
+
+#define fast_tracepoints_begin(tracepoint_name, start_timestamp)
+#define fast_tracepoint(tracepoint_name)
+#define fast_tracepoints_end(tracepoint_name)
+
+#endif /* CONFIG_FAST_TRACEPOINTS */
+
+#endif /* _TRACE_FAST_H */
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index a3f35c7d83b6..e93133ca8f4b 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -544,6 +544,15 @@ config MMIOTRACE
 	  See Documentation/trace/mmiotrace.rst.
 	  If you are not helping to develop drivers, say N.
 
+config FAST_TRACEPOINTS
+	bool "Fast tracepoints"
+	default n
+
+config FAST_TRACEPOINTS_PRINTK
+	bool "Use printk for fast tracepoints"
+	depends on FAST_TRACEPOINTS
+	default n
+
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index 057cd975d014..92cca87e1f70 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -106,6 +106,7 @@ obj-$(CONFIG_FTRACE_RECORD_RECURSION) += trace_recursion_record.o
 obj-$(CONFIG_FPROBE) += fprobe.o
 obj-$(CONFIG_RETHOOK) += rethook.o
 obj-$(CONFIG_FPROBE_EVENTS) += trace_fprobe.o
+obj-$(CONFIG_FAST_TRACEPOINTS) += fast.o
 
 obj-$(CONFIG_TRACEPOINT_BENCHMARK) += trace_benchmark.o
 obj-$(CONFIG_RV) += rv/
diff --git a/kernel/trace/fast.c b/kernel/trace/fast.c
new file mode 100644
index 000000000000..c8e076df1cfc
--- /dev/null
+++ b/kernel/trace/fast.c
@@ -0,0 +1,41 @@
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/sysctl.h>
+
+#include <trace/fast.h>
+
+#ifdef CONFIG_FAST_TRACEPOINTS_PRINTK
+
+const char *prev_fast_tracepoint_name;
+
+u64 prev_fast_tracepoint_timestamp;
+
+#else
+
+#define DEFINE_FAST_TRACEPOINT(name) u64 fast_tracepoint_timestamp_##name;
+ENUMERATE_FAST_TRACEPOINTS(DEFINE_FAST_TRACEPOINT)
+#undef DEFINE_FAST_TRACEPOINT
+
+static const struct ctl_table timestamp_ctls[] = {
+#define FAST_TRACEPOINT_CTL(name)                                                   \
+	{                                                           \
+		.procname = #name,                                  \
+		.data = &fast_tracepoint_timestamp_##name,          \
+		.maxlen = sizeof(fast_tracepoint_timestamp_##name), \
+		.mode = 0444,                                       \
+		.proc_handler = proc_doulongvec_minmax,             \
+	},
+	ENUMERATE_FAST_TRACEPOINTS(FAST_TRACEPOINT_CTL)
+#undef FAST_TRACEPOINT_CTL
+};
+
+static int __init fast_tracepoints_init(void)
+{
+	register_sysctl("debug/fast_tracepoints", timestamp_ctls);
+
+	return 0;
+}
+
+early_initcall(fast_tracepoints_init)
+
+#endif /* CONFIG_FAST_TRACEPOINTS_PRINTK */
diff --git a/mm/gup.c b/mm/gup.c
index 84461d384ae2..37c969a58b0b 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1190,7 +1190,7 @@ static int faultin_page(struct vm_area_struct *vma,
 		VM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
@@ -1650,7 +1650,7 @@ int fixup_user_fault(struct mm_struct *mm,
 	    fatal_signal_pending(current))
 		return -EINTR;
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 2a47682d1ab7..487b9a43f944 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1246,7 +1246,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			spin_unlock(vmf->ptl);
 			folio_put(folio);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -1355,7 +1355,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_folio(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index e3e6ac991b9c..68b6f2d7bb0c 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6367,7 +6367,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_fault *vmf,
 	hugetlb_vma_unlock_read(vmf->vma);
 	hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-	return handle_userfault(vmf, reason);
+	return handle_userfault(vmf, reason, NULL);
 }
 
 /*
@@ -6767,7 +6767,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			}
 			hugetlb_vma_unlock_read(vma);
 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-			return handle_userfault(&vmf, VM_UFFD_WP);
+			return handle_userfault(&vmf, VM_UFFD_WP, NULL);
 		}
 
 		vmf.orig_pte = huge_pte_clear_uffd_wp(vmf.orig_pte);
diff --git a/mm/ksm.c b/mm/ksm.c
index 8583fb91ef13..e7f517bcad8f 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -647,7 +647,7 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_v
 			return 0;
 		ret = handle_mm_fault(vma, addr,
 				      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,
-				      NULL);
+				      NULL, false);
 	} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop until we no longer find a KSM page because
diff --git a/mm/memory.c b/mm/memory.c
index ba3ea0a82f7f..c39a44c63469 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -78,6 +78,7 @@
 #include <linux/sched/sysctl.h>
 
 #include <trace/events/kmem.h>
+#include <trace/fast.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -85,6 +86,7 @@
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
+#include <asm/msr.h>
 
 #include "pgalloc-track.h"
 #include "internal.h"
@@ -95,7 +97,7 @@
 #endif
 
 static vm_fault_t do_fault(struct vm_fault *vmf);
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf);
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs, bool trace);
 static bool vmf_pte_changed(struct vm_fault *vmf);
 
 /*
@@ -3856,7 +3858,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 		if (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {
 			if (!userfaultfd_wp_async(vma)) {
 				pte_unmap_unlock(vmf->pte, vmf->ptl);
-				return handle_userfault(vmf, VM_UFFD_WP);
+				return handle_userfault(vmf, VM_UFFD_WP, NULL);
 			}
 
 			/*
@@ -4152,10 +4154,10 @@ static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
 	return 0;
 }
 
-static vm_fault_t do_pte_missing(struct vm_fault *vmf)
+static vm_fault_t do_pte_missing(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_anonymous_page(vmf);
+		return do_anonymous_page(vmf, regs, trace);
 	else
 		return do_fault(vmf);
 }
@@ -4173,7 +4175,7 @@ static vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)
 	if (unlikely(!userfaultfd_wp(vmf->vma)))
 		return pte_marker_clear(vmf);
 
-	return do_pte_missing(vmf);
+	return do_pte_missing(vmf, NULL, false);
 }
 
 static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
@@ -4948,7 +4950,7 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	unsigned long addr = vmf->address;
@@ -4987,7 +4989,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 		}
 		goto setpte;
 	}
@@ -5037,7 +5039,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		folio_put(folio);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 	}
 
 	folio_ref_add(folio, nr_pages - 1);
@@ -5877,7 +5879,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 		    userfaultfd_huge_pmd_wp(vma, vmf->orig_pmd)) {
 			if (userfaultfd_wp_async(vmf->vma))
 				goto split;
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, NULL);
 		}
 		return do_huge_pmd_wp_page(vmf);
 	}
@@ -5950,7 +5952,7 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	pte_t entry;
 
@@ -5993,8 +5995,15 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		}
 	}
 
+	if (likely(trace)) {
+		if (vmf->flags & FAULT_FLAG_VMA_LOCK)
+			fast_tracepoint(first_page_table_walk_end);
+		else
+			fast_tracepoint(second_page_table_walk_end);
+	}
+
 	if (!vmf->pte)
-		return do_pte_missing(vmf);
+		return do_pte_missing(vmf, regs, trace);
 
 	if (!pte_present(vmf->orig_pte))
 		return do_swap_page(vmf);
@@ -6045,7 +6054,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		struct pt_regs *regs, bool trace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -6137,7 +6147,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, regs, trace);
 }
 
 /**
@@ -6272,7 +6282,7 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-			   unsigned int flags, struct pt_regs *regs)
+			   unsigned int flags, struct pt_regs *regs, bool trace)
 {
 	/* If the fault handler drops the mmap_lock, vma may be freed */
 	struct mm_struct *mm = vma->vm_mm;
@@ -6306,7 +6316,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, regs, trace);
 
 	/*
 	 * Warning: It is no longer safe to dereference vma-> after this point,
diff --git a/mm/shmem.c b/mm/shmem.c
index 99327c30507c..2caba9796133 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2458,7 +2458,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	if (folio && vma && userfaultfd_minor(vma)) {
 		if (!xa_is_value(folio))
 			folio_put(folio);
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, NULL);
 		return 0;
 	}
 
@@ -2507,7 +2507,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 		return 0;
 	}
 
-- 
2.49.0

