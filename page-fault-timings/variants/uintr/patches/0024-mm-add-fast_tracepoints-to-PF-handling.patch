From b9f49a0eb24bbfb61240b4527823151f6af58c75 Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Tue, 13 May 2025 11:27:59 +0200
Subject: [PATCH 24/25] mm: add fast_tracepoints to PF handling

---
 arch/alpha/mm/fault.c         |  2 +-
 arch/arc/mm/fault.c           |  2 +-
 arch/arm/mm/fault.c           |  2 +-
 arch/arm64/mm/fault.c         |  2 +-
 arch/csky/mm/fault.c          |  2 +-
 arch/hexagon/mm/vm_fault.c    |  2 +-
 arch/ia64/mm/fault.c          |  2 +-
 arch/loongarch/mm/fault.c     |  2 +-
 arch/m68k/mm/fault.c          |  2 +-
 arch/microblaze/mm/fault.c    |  2 +-
 arch/nios2/mm/fault.c         |  2 +-
 arch/openrisc/mm/fault.c      |  2 +-
 arch/parisc/mm/fault.c        |  2 +-
 arch/powerpc/mm/copro_fault.c |  3 +-
 arch/powerpc/mm/fault.c       |  2 +-
 arch/riscv/mm/fault.c         |  2 +-
 arch/s390/mm/fault.c          |  2 +-
 arch/sh/mm/fault.c            |  2 +-
 arch/sparc/mm/fault_32.c      |  4 +--
 arch/sparc/mm/fault_64.c      |  2 +-
 arch/um/kernel/trap.c         |  2 +-
 arch/x86/mm/fault.c           | 33 +++++++++++++++++----
 arch/xtensa/mm/fault.c        |  2 +-
 drivers/iommu/amd/iommu_v2.c  |  2 +-
 drivers/iommu/io-pgfault.c    |  2 +-
 fs/userfaultfd.c              |  7 ++++-
 include/linux/mm.h            |  2 +-
 include/linux/userfaultfd_k.h |  2 +-
 include/trace/fast.h          | 18 +++++++++--
 mm/gup.c                      |  4 +--
 mm/hmm.c                      |  2 +-
 mm/huge_memory.c              |  4 +--
 mm/hugetlb.c                  |  4 +--
 mm/ksm.c                      |  2 +-
 mm/memory.c                   | 56 ++++++++++++++++++++++++-----------
 mm/shmem.c                    |  4 +--
 36 files changed, 127 insertions(+), 62 deletions(-)

diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
index ef427a6bdd1a..14972abf2242 100644
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@ -150,7 +150,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
 	/* If for any reason at all we couldn't handle the fault,
 	   make sure we exit gracefully rather than endlessly redo
 	   the fault.  */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c
index 5ca59a482632..69c1b9a3d43b 100644
--- a/arch/arc/mm/fault.c
+++ b/arch/arc/mm/fault.c
@@ -137,7 +137,7 @@ void do_page_fault(unsigned long address, struct pt_regs *regs)
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	/* Quick path to respond to signals */
 	if (fault_signal_pending(fault, regs)) {
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index de988cba9a4b..dc45c68b8611 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -255,7 +255,7 @@ __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int flags,
 	if (!(vma->vm_flags & vma_flags))
 		return VM_FAULT_BADACCESS;
 
-	return handle_mm_fault(vma, addr & PAGE_MASK, flags, regs);
+	return handle_mm_fault(vma, addr & PAGE_MASK, flags, regs, false);
 }
 
 static int __kprobes
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index 89628bd370d9..a9112318d155 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -505,7 +505,7 @@ static vm_fault_t __do_page_fault(struct mm_struct *mm, unsigned long addr,
 	 */
 	if (!(vma->vm_flags & vm_flags))
 		return VM_FAULT_BADACCESS;
-	return handle_mm_fault(vma, addr, mm_flags, regs);
+	return handle_mm_fault(vma, addr, mm_flags, regs, false);
 }
 
 static bool is_el0_instruction_abort(unsigned long esr)
diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index e15f736cca4b..d48d1b5584e1 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -272,7 +272,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c
index f73c7cbfe326..a9a9353fe1ef 100644
--- a/arch/hexagon/mm/vm_fault.c
+++ b/arch/hexagon/mm/vm_fault.c
@@ -91,7 +91,7 @@ void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)
 		break;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c
index ef78c2d66cdd..47d57d795e9d 100644
--- a/arch/ia64/mm/fault.c
+++ b/arch/ia64/mm/fault.c
@@ -134,7 +134,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re
 	 * sure we exit gracefully rather than endlessly redo the
 	 * fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/loongarch/mm/fault.c b/arch/loongarch/mm/fault.c
index 1ccd53655cab..f5fa6dc420f7 100644
--- a/arch/loongarch/mm/fault.c
+++ b/arch/loongarch/mm/fault.c
@@ -208,7 +208,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
index 4d2837eb3e2a..873fbdac33c1 100644
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@ -135,7 +135,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	pr_debug("handle_mm_fault returns %x\n", fault);
 
 	if (fault_signal_pending(fault, regs))
diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
index 5c40c3ebe52f..2c331615f213 100644
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@ -217,7 +217,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c
index edaca0a6c1c1..ac253fc3de49 100644
--- a/arch/nios2/mm/fault.c
+++ b/arch/nios2/mm/fault.c
@@ -134,7 +134,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c
index b4762d66e9ef..3aa71cdc3d4c 100644
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@ -160,7 +160,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c
index 869204e97ec9..0e0c65eb6552 100644
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@ -306,7 +306,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
 	 * fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c
index 7c507fb48182..8ab4c75273e5 100644
--- a/arch/powerpc/mm/copro_fault.c
+++ b/arch/powerpc/mm/copro_fault.c
@@ -64,7 +64,8 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,
 	}
 
 	ret = 0;
-	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL);
+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL,
+			       false);
 
 	/* The fault is fully completed (including releasing mmap lock) */
 	if (*flt & VM_FAULT_COMPLETED)
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 014005428687..c928aeb8b798 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -504,7 +504,7 @@ static int ___do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	major |= fault & VM_FAULT_MAJOR;
 
diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c
index d86f7cebd4a7..5aff25f999ad 100644
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@ -317,7 +317,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index 9ab6ca6f7f59..1d5a549370f7 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -426,7 +426,7 @@ static inline vm_fault_t do_exception(struct pt_regs *regs, int access)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	if (fault_signal_pending(fault, regs)) {
 		fault = VM_FAULT_SIGNAL;
 		if (flags & FAULT_FLAG_RETRY_NOWAIT)
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index acd2f5e50bfc..848a95043e15 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -479,7 +479,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (unlikely(fault & (VM_FAULT_RETRY | VM_FAULT_ERROR)))
 		if (mm_fault_error(regs, error_code, address, fault))
diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c
index 91259f291c54..e852be5e1c18 100644
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@ -185,7 +185,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		return;
@@ -338,7 +338,7 @@ static void force_user_fault(unsigned long address, int write)
 		if (!(vma->vm_flags & (VM_READ | VM_EXEC)))
 			goto bad_area;
 	}
-	switch (handle_mm_fault(vma, address, flags, NULL)) {
+	switch (handle_mm_fault(vma, address, flags, NULL, false)) {
 	case VM_FAULT_SIGBUS:
 	case VM_FAULT_OOM:
 		goto do_sigbus;
diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c
index 4acc12eafbf5..834b70a5dbfc 100644
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@ -422,7 +422,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
 			goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs))
 		goto exit_exception;
diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c
index d3ce21c4ca32..d8a1b01459ce 100644
--- a/arch/um/kernel/trap.c
+++ b/arch/um/kernel/trap.c
@@ -71,7 +71,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,
 	do {
 		vm_fault_t fault;
 
-		fault = handle_mm_fault(vma, address, flags, NULL);
+		fault = handle_mm_fault(vma, address, flags, NULL, false);
 
 		if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 			goto out_nosemaphore;
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 8e7107d37a87..50f8a352e10e 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1221,7 +1221,8 @@ NOKPROBE_SYMBOL(do_kern_addr_fault);
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
 			unsigned long error_code,
-			unsigned long address)
+			unsigned long address,
+			bool trace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
@@ -1337,6 +1338,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * 1. Failed to acquire mmap_lock, and
 	 * 2. The access did not originate in userspace.
 	 */
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(first_try_lock_mm_and_find_vma_start);
 	if (unlikely(!mmap_read_trylock(mm))) {
 		if (!user_mode(regs) && !search_exception_tables(regs->ip)) {
 			/*
@@ -1347,6 +1350,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 			return;
 		}
 retry:
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace) && (flags & FAULT_FLAG_TRIED))
+			fast_tracepoint(retry_lock_mm_and_find_vma_start);
 		mmap_read_lock(mm);
 	} else {
 		/*
@@ -1358,6 +1363,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 	}
 
 	vma = find_vma(mm, address);
+	if (IS_ENABLED(CONFIG_TRACE_PF)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_lock_mm_and_find_vma_start);
+		else
+			fast_tracepoint(first_try_lock_mm_and_find_vma_start);
+	}
 	if (unlikely(!vma)) {
 		bad_area(regs, error_code, address);
 		return;
@@ -1396,7 +1407,19 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_start);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_start);
+	}
+	fault = handle_mm_fault(vma, address, flags, regs, trace);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_end);
+	}
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
@@ -1476,7 +1499,7 @@ trace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,
 
 static __always_inline void
 handle_page_fault(struct pt_regs *regs, unsigned long error_code,
-			      unsigned long address)
+			      unsigned long address, bool trace)
 {
 	trace_page_fault_entries(regs, error_code, address);
 
@@ -1487,7 +1510,7 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, error_code, address);
 	} else {
-		do_user_addr_fault(regs, error_code, address);
+		do_user_addr_fault(regs, error_code, address, trace);
 		/*
 		 * User address page fault handling might have reenabled
 		 * interrupts. Fixing up all potential exit points of
@@ -1547,7 +1570,7 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	state = irqentry_enter(regs);
 
 	instrumentation_begin();
-	handle_page_fault(regs, error_code, address);
+	handle_page_fault(regs, error_code, address, trace);
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
index 8c781b05c0bd..ccdc5e03f5aa 100644
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@ -164,7 +164,7 @@ void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/drivers/iommu/amd/iommu_v2.c b/drivers/iommu/amd/iommu_v2.c
index 9f7fab49a5a9..e3d6f34f6cb6 100644
--- a/drivers/iommu/amd/iommu_v2.c
+++ b/drivers/iommu/amd/iommu_v2.c
@@ -494,7 +494,7 @@ static void do_fault(struct work_struct *work)
 	if (access_error(vma, fault))
 		goto out;
 
-	ret = handle_mm_fault(vma, address, flags, NULL);
+	ret = handle_mm_fault(vma, address, flags, NULL, false);
 out:
 	mmap_read_unlock(mm);
 
diff --git a/drivers/iommu/io-pgfault.c b/drivers/iommu/io-pgfault.c
index 1df8c1dcae77..fc6472a643e1 100644
--- a/drivers/iommu/io-pgfault.c
+++ b/drivers/iommu/io-pgfault.c
@@ -114,7 +114,7 @@ iopf_handle_single(struct iopf_fault *iopf)
 		/* Access fault */
 		goto out_put_mm;
 
-	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL);
+	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL, false);
 	status = ret & VM_FAULT_ERROR ? IOMMU_PAGE_RESP_INVALID :
 		IOMMU_PAGE_RESP_SUCCESS;
 
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 0c1d33c4f74c..1938d7f35539 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -30,6 +30,7 @@
 #include <linux/security.h>
 #include <linux/hugetlb.h>
 #include <linux/swapops.h>
+#include <trace/fast.h>
 
 int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -373,7 +374,7 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace)
 {
 	struct mm_struct *mm = vmf->vma->vm_mm;
 	struct userfaultfd_ctx *ctx;
@@ -521,7 +522,11 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	mmap_read_unlock(mm);
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_start);
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_end);
 		schedule();
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 4ff52127a6b8..5082a71f7a85 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -1843,7 +1843,7 @@ int generic_error_remove_page(struct address_space *mapping, struct page *page);
 #ifdef CONFIG_MMU
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
 				  unsigned long address, unsigned int flags,
-				  struct pt_regs *regs);
+				  struct pt_regs *regs, bool trace);
 extern int fixup_user_fault(struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 31d86b8c0634..2c6963693440 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -38,7 +38,7 @@
 
 extern int sysctl_unprivileged_userfaultfd;
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace);
 
 /*
  * The mode of operation for __mcopy_atomic and its helpers.
diff --git a/include/trace/fast.h b/include/trace/fast.h
index 77730c143325..b5ad1f8cb29a 100644
--- a/include/trace/fast.h
+++ b/include/trace/fast.h
@@ -9,8 +9,22 @@
 
 #include <asm/msr.h>
 
-#define ENUMERATE_FAST_TRACEPOINTS(x) \
-	x(c_entry)		      \
+#define ENUMERATE_FAST_TRACEPOINTS(x)				      \
+	x(c_entry)						      \
+	x(first_try_lock_mm_and_find_vma_start)			      \
+	x(first_try_lock_mm_and_find_vma_end)			      \
+	x(first_try_handle_mm_fault_with_mm_lock_start)		      \
+	x(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end) \
+	x(page_allocation_start)				      \
+	x(page_allocation_end)					      \
+	x(wake_up_userfaultfd_start)				      \
+	x(wake_up_userfaultfd_end)				      \
+	x(first_try_handle_mm_fault_with_mm_lock_end)		      \
+	x(retry_lock_mm_and_find_vma_start)			      \
+	x(retry_lock_mm_and_find_vma_end)			      \
+	x(retry_handle_mm_fault_with_mm_lock_start)		      \
+	x(retry_handle_mm_fault_with_mm_lock_page_table_walk_end)     \
+	x(retry_handle_mm_fault_with_mm_lock_end)		      \
 	x(c_exit)
 
 #define DECLARE_FAST_TRACEPOINT(name) extern u64 fast_tracepoint_##name;
diff --git a/mm/gup.c b/mm/gup.c
index 60d7213ad95b..dfba62bda41b 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -999,7 +999,7 @@ static int faultin_page(struct vm_area_struct *vma,
 		VM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
@@ -1363,7 +1363,7 @@ int fixup_user_fault(struct mm_struct *mm,
 	    fatal_signal_pending(current))
 		return -EINTR;
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
diff --git a/mm/hmm.c b/mm/hmm.c
index f2aa63b94d9b..dc90bdbd06dd 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -77,7 +77,7 @@ static int hmm_vma_fault(unsigned long addr, unsigned long end,
 	}
 
 	for (; addr < end; addr += PAGE_SIZE)
-		if (handle_mm_fault(vma, addr, fault_flags, NULL) &
+		if (handle_mm_fault(vma, addr, fault_flags, NULL, false) &
 		    VM_FAULT_ERROR)
 			return -EFAULT;
 	return -EBUSY;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 9558dbf3954c..bc4758ee74bb 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -697,7 +697,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf,
 			spin_unlock(vmf->ptl);
 			put_page(page);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -816,7 +816,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_page(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 022a3bfafec4..9cb14bf491e9 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -5509,7 +5509,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_area_struct *vma,
 	hash = hugetlb_fault_mutex_hash(mapping, idx);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
 	i_mmap_unlock_read(mapping);
-	return handle_userfault(&vmf, reason);
+	return handle_userfault(&vmf, reason, false);
 }
 
 static vm_fault_t hugetlb_no_page(struct mm_struct *mm,
@@ -5840,7 +5840,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 		}
 		mutex_unlock(&hugetlb_fault_mutex_table[hash]);
 		i_mmap_unlock_read(mapping);
-		return handle_userfault(&vmf, VM_UFFD_WP);
+		return handle_userfault(&vmf, VM_UFFD_WP, false);
 	}
 
 	/*
diff --git a/mm/ksm.c b/mm/ksm.c
index 42ab153335a2..6853a8e70930 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -480,7 +480,7 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)
 		if (PageKsm(page))
 			ret = handle_mm_fault(vma, addr,
 					      FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,
-					      NULL);
+					      NULL, false);
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
diff --git a/mm/memory.c b/mm/memory.c
index a0fdaa74091f..673d1a2dbcb8 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -76,6 +76,7 @@
 #include <linux/vmalloc.h>
 
 #include <trace/events/kmem.h>
+#include <trace/fast.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -83,6 +84,7 @@
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
+#include <asm/msr.h>
 
 #include "pgalloc-track.h"
 #include "internal.h"
@@ -3088,7 +3090,7 @@ static inline void wp_page_reuse(struct vm_fault *vmf)
  *   held to the old page, as well as updating the rmap.
  * - In any case, unlock the PTL and drop the reference we took to the old page.
  */
-static vm_fault_t wp_page_copy(struct vm_fault *vmf)
+static vm_fault_t wp_page_copy(struct vm_fault *vmf, bool trace)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
 	struct vm_area_struct *vma = vmf->vma;
@@ -3105,13 +3107,21 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		goto oom;
 
 	if (is_zero_pfn(pte_pfn(vmf->orig_pte))) {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_start);
 		new_page = alloc_zeroed_user_highpage_movable(vma,
 							      vmf->address);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_end);
 		if (!new_page)
 			goto oom;
 	} else {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_start);
 		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma,
 				vmf->address);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(page_allocation_end);
 		if (!new_page)
 			goto oom;
 
@@ -3358,7 +3368,7 @@ static vm_fault_t wp_page_shared(struct vm_fault *vmf)
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_wp_page(struct vm_fault *vmf)
+static vm_fault_t do_wp_page(struct vm_fault *vmf, bool trace)
 	__releases(vmf->ptl)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
@@ -3370,7 +3380,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (likely(!unshare)) {
 		if (userfaultfd_pte_wp(vma, *vmf->pte)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, trace);
 		}
 
 		/*
@@ -3402,7 +3412,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 			return wp_pfn_shared(vmf);
 
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
-		return wp_page_copy(vmf);
+		return wp_page_copy(vmf, trace);
 	}
 
 	/*
@@ -3477,7 +3487,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (PageKsm(vmf->page))
 		count_vm_event(COW_KSM);
 #endif
-	return wp_page_copy(vmf);
+	return wp_page_copy(vmf, trace);
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
@@ -3995,7 +4005,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		ret |= do_wp_page(vmf);
+		ret |= do_wp_page(vmf, false);
 		if (ret & VM_FAULT_ERROR)
 			ret &= VM_FAULT_ERROR;
 		goto out;
@@ -4029,7 +4039,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct page *page;
@@ -4074,7 +4084,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 		}
 		goto setpte;
 	}
@@ -4082,7 +4092,11 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	/* Allocate our own private page. */
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_start);
 	page = alloc_zeroed_user_highpage_movable(vma, vmf->address);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_end);
 	if (!page)
 		goto oom;
 
@@ -4117,7 +4131,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		put_page(page);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 	}
 
 	inc_mm_counter_fast(vma->vm_mm, MM_ANONPAGES);
@@ -4794,7 +4808,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 	if (vma_is_anonymous(vmf->vma)) {
 		if (likely(!unshare) &&
 		    userfaultfd_huge_pmd_wp(vmf->vma, vmf->orig_pmd))
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, false);
 		return do_huge_pmd_wp_page(vmf);
 	}
 	if (vmf->vma->vm_ops->huge_fault) {
@@ -4858,7 +4872,7 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, bool trace)
 {
 	pte_t entry;
 
@@ -4911,9 +4925,16 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		}
 	}
 
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (vmf->flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_page_table_walk_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end);
+	}
+
 	if (!vmf->pte) {
 		if (vma_is_anonymous(vmf->vma))
-			return do_anonymous_page(vmf);
+			return do_anonymous_page(vmf, trace);
 		else
 			return do_fault(vmf);
 	}
@@ -4933,7 +4954,7 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 	if (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {
 		if (!pte_write(entry))
-			return do_wp_page(vmf);
+			return do_wp_page(vmf, trace);
 		else if (likely(vmf->flags & FAULT_FLAG_WRITE))
 			entry = pte_mkdirty(entry);
 	}
@@ -4966,7 +4987,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		bool trace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -5057,7 +5079,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, trace);
 }
 
 /**
@@ -5128,7 +5150,7 @@ static inline void mm_account_fault(struct pt_regs *regs,
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-			   unsigned int flags, struct pt_regs *regs)
+			   unsigned int flags, struct pt_regs *regs, bool trace)
 {
 	vm_fault_t ret;
 
@@ -5155,7 +5177,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, trace);
 
 	if (flags & FAULT_FLAG_USER) {
 		mem_cgroup_exit_user_fault();
diff --git a/mm/shmem.c b/mm/shmem.c
index 112ebf601bb4..2e646a0ef099 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1875,7 +1875,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 			folio_unlock(folio);
 			folio_put(folio);
 		}
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, false);
 		return 0;
 	}
 
@@ -1917,7 +1917,7 @@ static int shmem_getpage_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, false);
 		return 0;
 	}
 
-- 
2.49.0

