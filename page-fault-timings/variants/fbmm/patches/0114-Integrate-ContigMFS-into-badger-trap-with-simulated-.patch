From 91d701cd2d35c32acf9fd63ace1b8b9ce8e434a5 Mon Sep 17 00:00:00 2001
From: BIJAN TABATABAI <btabatabai@wisc.edu>
Date: Fri, 27 Oct 2023 11:11:17 -0500
Subject: [PATCH 114/179] Integrate ContigMFS into badger trap with simulated
 range-tlb

---
 ContigMMFS/contig.c      | 20 +++++++++++++----
 include/linux/mm_types.h | 13 ++++++++++++
 include/linux/sched.h    |  1 +
 kernel/exit.c            |  3 +++
 kernel/fork.c            | 14 ++++++++++++
 mm/memory.c              | 46 +++++++++++++++++++++++++++++++++++-----
 6 files changed, 88 insertions(+), 9 deletions(-)

diff --git a/ContigMMFS/contig.c b/ContigMMFS/contig.c
index fc361f66373c..fbd4d2e72046 100644
--- a/ContigMMFS/contig.c
+++ b/ContigMMFS/contig.c
@@ -48,9 +48,7 @@ static vm_fault_t contigmmfs_fault(struct vm_fault *vmf)
 
     // For now, do nothing if the pte already exists
     if (vmf->pte) {
-        vmf->page = page;
-        vmf->orig_pte = *vmf->pte;
-        return 0;
+        return VM_FAULT_NOPAGE;
     }
 
     if (pte_alloc(vma->vm_mm, vmf->pmd))
@@ -69,6 +67,7 @@ static vm_fault_t contigmmfs_fault(struct vm_fault *vmf)
     }
 
     page_add_file_rmap(page, vma, false);
+    percpu_counter_inc(&vma->vm_mm->rss_stat[MM_FILEPAGES]);
     set_pte_at(vma->vm_mm, vmf->address, vmf->pte, entry);
     folio_get(region->folio);
 
@@ -92,6 +91,7 @@ static int contigmmfs_mmap(struct file *file, struct vm_area_struct *vma)
     struct contigmmfs_inode_info *inode_info = CMMFS_I(inode);
     struct contigmmfs_sb_info *sbi = CMMFS_SB(inode->i_sb);
     struct contigmmfs_contig_alloc *region = NULL;
+    struct range_tlb_entry *tlb_entry = NULL;
     struct folio *new_folio = NULL;
     u64 pages_to_alloc = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
     u64 current_va = vma->vm_start;
@@ -142,6 +142,18 @@ static int contigmmfs_mmap(struct file *file, struct vm_area_struct *vma)
         if(mtree_store(&inode_info->mt, current_va, region, GFP_KERNEL))
             goto err;
 
+        // If badger trap is being used, add the ranges to the mm's list
+        if (vma->vm_mm && vma->vm_mm->badger_trap_en && folio_size >= 8) {
+            tlb_entry = kzalloc(sizeof(struct range_tlb_entry), GFP_KERNEL);
+            // I'm being lazy here without the error checking, but it's
+            // *probably* fine
+            tlb_entry->range_start = region->va_start;
+            tlb_entry->range_end = region->va_end;
+            spin_lock(&vma->vm_mm->range_tlb_lock);
+            mtree_store(&vma->vm_mm->all_ranges, tlb_entry->range_start, tlb_entry, GFP_KERNEL);
+            spin_unlock(&vma->vm_mm->range_tlb_lock);
+        }
+
         // TODO: It would probably be good to setup the page tables here,
         // but it's easier if we just let the page fault handler to 90% of the
         // work then do the rest in the page fault callback
@@ -167,7 +179,7 @@ static int contigmmfs_mmap(struct file *file, struct vm_area_struct *vma)
 
 static long contigmmfs_fallocate(struct file *file, int mode, loff_t offset, loff_t len)
 {
-    return -EOPNOTSUPP;
+    return 0;
 }
 
 const struct file_operations contigmmfs_file_operations = {
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 75348feac477..3767dd570c81 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -598,6 +598,14 @@ struct vm_area_struct {
 } __randomize_layout;
 
 struct kioctx_table;
+
+#define MAX_RANGE_TLB_ENTRIES 32
+struct range_tlb_entry {
+	u64 range_start;
+	u64 range_end;
+	struct list_head node;
+};
+
 struct mm_struct {
 	struct {
 		struct maple_tree mm_mt;
@@ -810,6 +818,11 @@ struct mm_struct {
 	unsigned long total_dtlb_misses;
 	unsigned long total_dtlb_4k_misses;
 	unsigned long total_dtlb_hugetlb_misses;
+	unsigned long total_range_tlb_hits;
+	spinlock_t range_tlb_lock;
+	struct list_head range_tlb;
+	struct maple_tree all_ranges;
+	unsigned long range_tlb_size;
 
 	/*
 	 * The mm_cpumask needs to be at the end of mm_struct, because it
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ba3e877099dd..e8a4ef020447 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1531,6 +1531,7 @@ struct task_struct {
 	unsigned long total_dtlb_misses;
 	unsigned long total_dtlb_4k_misses;
 	unsigned long total_dtlb_hugetlb_misses;
+	unsigned long total_range_tlb_hits;
 
 	/* CPU-specific state of this task: */
 	struct thread_struct		thread;
diff --git a/kernel/exit.c b/kernel/exit.c
index 3301473ca1fd..5f67d349df19 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -824,6 +824,7 @@ void __noreturn do_exit(long code)
 		current->mm->total_dtlb_misses+= current->total_dtlb_misses;
 		current->mm->total_dtlb_4k_misses+= current->total_dtlb_4k_misses;
 		current->mm->total_dtlb_hugetlb_misses+= current->total_dtlb_hugetlb_misses;
+		current->mm->total_range_tlb_hits += current->total_range_tlb_hits;
 		mutex_unlock(&result_mutex);
 	}
 
@@ -835,6 +836,7 @@ void __noreturn do_exit(long code)
 			current->real_parent->mm->total_dtlb_misses+=current->mm->total_dtlb_misses;
 			current->real_parent->mm->total_dtlb_4k_misses+=current->mm->total_dtlb_4k_misses;
 			current->real_parent->mm->total_dtlb_hugetlb_misses+=current->mm->total_dtlb_hugetlb_misses;
+			current->real_parent->mm->total_range_tlb_hits += current->mm->total_range_tlb_hits;
 			mutex_unlock(&result_mutex);
 		}
 		else
@@ -844,6 +846,7 @@ void __noreturn do_exit(long code)
 			printk("DTLB miss detected %lu\n",current->mm->total_dtlb_misses);
 			printk("DTLB miss for 4KB page detected %lu\n",current->mm->total_dtlb_4k_misses);
 			printk("DTLB miss for hugepage detected %lu\n",current->mm->total_dtlb_hugetlb_misses);
+			printk("Range TLB hit detected %lu\n", current->mm->total_range_tlb_hits);
 			printk("===================================\n");
 		}
 	}
diff --git a/kernel/fork.c b/kernel/fork.c
index b082628db9f8..42a6f82314e3 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -1171,6 +1171,12 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p,
 		if (percpu_counter_init(&mm->rss_stat[i], 0, GFP_KERNEL_ACCOUNT))
 			goto fail_pcpu;
 
+	// Bijan: This is for badger trap with the ContigMMFS
+	spin_lock_init(&mm->range_tlb_lock);
+	INIT_LIST_HEAD(&mm->range_tlb);
+	mt_init(&mm->all_ranges);
+	mm->range_tlb_size = 0;
+
 	mm->user_ns = get_user_ns(user_ns);
 	lru_gen_init_mm(mm);
 	return mm;
@@ -1203,6 +1209,8 @@ struct mm_struct *mm_alloc(void)
 
 static inline void __mmput(struct mm_struct *mm)
 {
+	struct range_tlb_entry *tlb_entry;
+	unsigned long index = 0;
 	VM_BUG_ON(atomic_read(&mm->mm_users));
 
 	uprobe_clear_state(mm);
@@ -1220,6 +1228,12 @@ static inline void __mmput(struct mm_struct *mm)
 	if (mm->binfmt)
 		module_put(mm->binfmt->module);
 	lru_gen_del_mm(mm);
+
+	mt_for_each(&mm->all_ranges, tlb_entry, index, ULONG_MAX) {
+		kfree(tlb_entry);
+	}
+	mtree_destroy(&mm->all_ranges);
+
 	mmdrop(mm);
 }
 
diff --git a/mm/memory.c b/mm/memory.c
index 61ce60d05bf7..7633f86f396f 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -4208,7 +4208,9 @@ static vm_fault_t __do_fault(struct vm_fault *vmf)
 	}
 
 	ret = vma->vm_ops->fault(vmf);
-	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |
+	if (unlikely(ret & VM_FAULT_NOPAGE))
+		goto badger_trap;
+	else if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE | VM_FAULT_RETRY |
 			    VM_FAULT_DONE_COW)))
 		return ret;
 
@@ -4229,6 +4231,7 @@ static vm_fault_t __do_fault(struct vm_fault *vmf)
 		return poisonret;
 	}
 
+badger_trap:
 	/* Make the page table entry as reserved for TLB miss tracking */
 	if(vma->vm_mm && (vma->vm_mm->badger_trap_en==1) && (!(vmf->flags & FAULT_FLAG_INSTRUCTION))) {
 		ptep = pte_offset_map_lock(vma->vm_mm, vmf->pmd, vmf->address, &vmf->ptl);
@@ -4240,10 +4243,12 @@ static vm_fault_t __do_fault(struct vm_fault *vmf)
     	pte_unmap_unlock(ptep, vmf->ptl);
 	}
 
-	if (unlikely(!(ret & VM_FAULT_LOCKED)))
-		lock_page(vmf->page);
-	else
-		VM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);
+	if (!(ret & VM_FAULT_NOPAGE)) {
+		if (unlikely(!(ret & VM_FAULT_LOCKED)))
+			lock_page(vmf->page);
+		else
+			VM_BUG_ON_PAGE(!PageLocked(vmf->page), vmf->page);
+	}
 
 	return ret;
 }
@@ -4924,6 +4929,8 @@ static int do_fake_page_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 {
         unsigned long *touch_page_addr;
         unsigned long touched;
+	struct range_tlb_entry *tlb_entry, *tmp;
+	bool range_tlb_hit = false;
 	unsigned long ret;
 	static unsigned int consecutive = 0;
 	static unsigned long prev_address = 0;
@@ -4959,6 +4966,35 @@ static int do_fake_page_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 	current->total_dtlb_4k_misses++;
 	current->total_dtlb_misses++;
 
+	// Check for range tlb stuff
+	spin_lock(&mm->range_tlb_lock);
+	list_for_each_entry_safe(tlb_entry, tmp, &mm->range_tlb, node) {
+		if (tlb_entry->range_start <= address && address < tlb_entry->range_end) {
+			current->total_range_tlb_hits++;
+
+			// Move this range to the head of the list, to simulate LRU eviction
+			list_del(&tlb_entry->node);
+			list_add(&tlb_entry->node, &mm->range_tlb);
+
+			range_tlb_hit = true;
+			break;
+		}
+	}
+
+	// If we didn't hit in the range tlb, see if the range exists at all
+	if (!range_tlb_hit) {
+		tlb_entry = mt_prev(&mm->all_ranges, address + 1, 0);
+		if (tlb_entry && tlb_entry->range_start <= address && address < tlb_entry->range_end) {
+			// Evict the LRU entry if the tlb is full and insert the new one into the range tlb
+			if (mm->range_tlb_size >= MAX_RANGE_TLB_ENTRIES)
+				list_del(mm->range_tlb.prev);
+			else
+				mm->range_tlb_size++;
+			list_add(&tlb_entry->node, &mm->range_tlb);
+		}
+	}
+	spin_unlock(&mm->range_tlb_lock);
+
 	*page_table = pte_mkreserve(*page_table);
 	pte_unmap_unlock(page_table, ptl);
 
-- 
2.49.0

