From b64f7306173517eee26807eaa9565c2437070a61 Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Tue, 13 May 2025 11:27:59 +0200
Subject: [PATCH 4/5] mm: add fast tracepoints to PF handling

---
 arch/alpha/mm/fault.c         |  2 +-
 arch/arc/mm/fault.c           |  2 +-
 arch/arm/mm/fault.c           |  5 +--
 arch/arm64/mm/fault.c         |  5 +--
 arch/csky/mm/fault.c          |  2 +-
 arch/hexagon/mm/vm_fault.c    |  2 +-
 arch/loongarch/mm/fault.c     |  2 +-
 arch/m68k/mm/fault.c          |  2 +-
 arch/microblaze/mm/fault.c    |  2 +-
 arch/nios2/mm/fault.c         |  2 +-
 arch/openrisc/mm/fault.c      |  2 +-
 arch/parisc/mm/fault.c        |  2 +-
 arch/powerpc/mm/copro_fault.c |  3 +-
 arch/powerpc/mm/fault.c       |  5 +--
 arch/riscv/mm/fault.c         |  5 +--
 arch/s390/mm/fault.c          |  5 +--
 arch/s390/mm/gmap_helpers.c   |  2 +-
 arch/sh/mm/fault.c            |  2 +-
 arch/sparc/mm/fault_32.c      |  4 +--
 arch/sparc/mm/fault_64.c      |  2 +-
 arch/um/kernel/trap.c         |  2 +-
 arch/x86/mm/fault.c           | 47 ++++++++++++++++++++++++----
 arch/xtensa/mm/fault.c        |  2 +-
 drivers/iommu/iommu-sva.c     |  2 +-
 fs/userfaultfd.c              |  7 ++++-
 include/linux/mm.h            |  2 +-
 include/linux/userfaultfd_k.h |  2 +-
 include/trace/fast.h          | 23 ++++++++++++--
 mm/gup.c                      |  4 +--
 mm/hmm.c                      |  2 +-
 mm/huge_memory.c              |  4 +--
 mm/hugetlb.c                  |  4 +--
 mm/ksm.c                      |  2 +-
 mm/memory.c                   | 59 +++++++++++++++++++++++------------
 mm/shmem.c                    |  4 +--
 35 files changed, 154 insertions(+), 70 deletions(-)

diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c
index a9816bbc9f34..abb161214ea3 100644
--- a/arch/alpha/mm/fault.c
+++ b/arch/alpha/mm/fault.c
@@ -142,7 +142,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,
 	/* If for any reason at all we couldn't handle the fault,
 	   make sure we exit gracefully rather than endlessly redo
 	   the fault.  */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c
index 95119a5e7761..14575383256b 100644
--- a/arch/arc/mm/fault.c
+++ b/arch/arc/mm/fault.c
@@ -132,7 +132,7 @@ void do_page_fault(unsigned long address, struct pt_regs *regs)
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	/* Quick path to respond to signals */
 	if (fault_signal_pending(fault, regs)) {
diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c
index ab01b51de559..f7c02d6f9461 100644
--- a/arch/arm/mm/fault.c
+++ b/arch/arm/mm/fault.c
@@ -325,7 +325,8 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 		code = SEGV_ACCERR;
 		goto bad_area;
 	}
-	fault = handle_mm_fault(vma, addr, flags | FAULT_FLAG_VMA_LOCK, regs);
+	fault = handle_mm_fault(vma, addr, flags | FAULT_FLAG_VMA_LOCK, regs,
+				false);
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -364,7 +365,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, addr & PAGE_MASK, flags, regs);
+	fault = handle_mm_fault(vma, addr & PAGE_MASK, flags, regs, false);
 
 	/* If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_lock because
diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c
index ec0a337891dd..966ed59917b5 100644
--- a/arch/arm64/mm/fault.c
+++ b/arch/arm64/mm/fault.c
@@ -644,7 +644,8 @@ static int __kprobes do_page_fault(unsigned long far, unsigned long esr,
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, addr, mm_flags | FAULT_FLAG_VMA_LOCK, regs);
+	fault = handle_mm_fault(vma, addr, mm_flags | FAULT_FLAG_VMA_LOCK,
+				regs, false);
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -687,7 +688,7 @@ static int __kprobes do_page_fault(unsigned long far, unsigned long esr,
 		goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, addr, mm_flags, regs);
+	fault = handle_mm_fault(vma, addr, mm_flags, regs, false);
 
 	/* Quick path to respond to signals */
 	if (fault_signal_pending(fault, regs)) {
diff --git a/arch/csky/mm/fault.c b/arch/csky/mm/fault.c
index a885518ce1dd..37c028a2425b 100644
--- a/arch/csky/mm/fault.c
+++ b/arch/csky/mm/fault.c
@@ -260,7 +260,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c
index 3771fb453898..21fb2817b19d 100644
--- a/arch/hexagon/mm/vm_fault.c
+++ b/arch/hexagon/mm/vm_fault.c
@@ -81,7 +81,7 @@ static void do_page_fault(unsigned long address, long cause, struct pt_regs *reg
 		break;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/loongarch/mm/fault.c b/arch/loongarch/mm/fault.c
index deefd9617d00..646f71514e1b 100644
--- a/arch/loongarch/mm/fault.c
+++ b/arch/loongarch/mm/fault.c
@@ -254,7 +254,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c
index fa3c5f38d989..969c110752b3 100644
--- a/arch/m68k/mm/fault.c
+++ b/arch/m68k/mm/fault.c
@@ -138,7 +138,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	pr_debug("handle_mm_fault returns %x\n", fault);
 
 	if (fault_signal_pending(fault, regs)) {
diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c
index d3c3c33b73a6..a53c2ad9d03f 100644
--- a/arch/microblaze/mm/fault.c
+++ b/arch/microblaze/mm/fault.c
@@ -218,7 +218,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c
index e3fa9c15181d..96704ef5d1f7 100644
--- a/arch/nios2/mm/fault.c
+++ b/arch/nios2/mm/fault.c
@@ -121,7 +121,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c
index 29e232d78d82..caf9d44a7cca 100644
--- a/arch/openrisc/mm/fault.c
+++ b/arch/openrisc/mm/fault.c
@@ -163,7 +163,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * the fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c
index c39de84e98b0..c3ea297f868f 100644
--- a/arch/parisc/mm/fault.c
+++ b/arch/parisc/mm/fault.c
@@ -315,7 +315,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,
 	 * fault.
 	 */
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs)) {
diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c
index f5f8692e2c69..ac9aab929211 100644
--- a/arch/powerpc/mm/copro_fault.c
+++ b/arch/powerpc/mm/copro_fault.c
@@ -54,7 +54,8 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,
 	}
 
 	ret = 0;
-	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL);
+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0, NULL,
+			       false);
 
 	/* The fault is fully completed (including releasing mmap lock) */
 	if (*flt & VM_FAULT_COMPLETED)
diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c
index 806c74e0d5ab..979613fe07e3 100644
--- a/arch/powerpc/mm/fault.c
+++ b/arch/powerpc/mm/fault.c
@@ -502,7 +502,8 @@ static int ___do_page_fault(struct pt_regs *regs, unsigned long address,
 		return bad_access(regs, address, NULL, vma);
 	}
 
-	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);
+	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK,
+				regs, false);
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -545,7 +546,7 @@ static int ___do_page_fault(struct pt_regs *regs, unsigned long address,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	major |= fault & VM_FAULT_MAJOR;
 
diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c
index 0194324a0c50..12986e978a6b 100644
--- a/arch/riscv/mm/fault.c
+++ b/arch/riscv/mm/fault.c
@@ -351,7 +351,8 @@ void handle_page_fault(struct pt_regs *regs)
 		return;
 	}
 
-	fault = handle_mm_fault(vma, addr, flags | FAULT_FLAG_VMA_LOCK, regs);
+	fault = handle_mm_fault(vma, addr, flags | FAULT_FLAG_VMA_LOCK, regs,
+				false);
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -395,7 +396,7 @@ void handle_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, addr, flags, regs);
+	fault = handle_mm_fault(vma, addr, flags, regs, false);
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c
index e1ad05bfd28a..12ed73faaf68 100644
--- a/arch/s390/mm/fault.c
+++ b/arch/s390/mm/fault.c
@@ -294,7 +294,8 @@ static void do_exception(struct pt_regs *regs, int access)
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
 		return handle_fault_error_nolock(regs, SEGV_ACCERR);
 	}
-	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);
+	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK,
+				regs, false);
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 	if (!(fault & VM_FAULT_RETRY)) {
@@ -317,7 +318,7 @@ static void do_exception(struct pt_regs *regs, int access)
 		return handle_fault_error_nolock(regs, SEGV_MAPERR);
 	if (unlikely(!(vma->vm_flags & access)))
 		return handle_fault_error(regs, SEGV_ACCERR);
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
 			handle_fault_error_nolock(regs, 0);
diff --git a/arch/s390/mm/gmap_helpers.c b/arch/s390/mm/gmap_helpers.c
index a45d417ad951..5fc8b85023e5 100644
--- a/arch/s390/mm/gmap_helpers.c
+++ b/arch/s390/mm/gmap_helpers.c
@@ -163,7 +163,7 @@ static int __gmap_helper_unshare_zeropages(struct mm_struct *mm)
 		/* addr was updated by find_zeropage_pte_entry() */
 		fault = handle_mm_fault(vma, addr,
 					FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,
-					NULL);
+					NULL, false);
 		if (fault & VM_FAULT_OOM)
 			return -ENOMEM;
 		/*
diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c
index 06e6b4952924..a45fae35c05c 100644
--- a/arch/sh/mm/fault.c
+++ b/arch/sh/mm/fault.c
@@ -466,7 +466,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (unlikely(fault & (VM_FAULT_RETRY | VM_FAULT_ERROR)))
 		if (mm_fault_error(regs, error_code, address, fault))
diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c
index 86a831ebd8c8..48d0dba37e9c 100644
--- a/arch/sparc/mm/fault_32.c
+++ b/arch/sparc/mm/fault_32.c
@@ -176,7 +176,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!from_user)
@@ -324,7 +324,7 @@ static void force_user_fault(unsigned long address, int write)
 		if (!(vma->vm_flags & (VM_READ | VM_EXEC)))
 			goto bad_area;
 	}
-	switch (handle_mm_fault(vma, address, flags, NULL)) {
+	switch (handle_mm_fault(vma, address, flags, NULL, false)) {
 	case VM_FAULT_SIGBUS:
 	case VM_FAULT_OOM:
 		goto do_sigbus;
diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c
index e326caf708c6..2fa58549a5b7 100644
--- a/arch/sparc/mm/fault_64.c
+++ b/arch/sparc/mm/fault_64.c
@@ -426,7 +426,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)
 			goto bad_area;
 	}
 
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (regs->tstate & TSTATE_PRIV) {
diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c
index 5b80a3a89c20..c9c0844082e2 100644
--- a/arch/um/kernel/trap.c
+++ b/arch/um/kernel/trap.c
@@ -177,7 +177,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,
 	do {
 		vm_fault_t fault;
 
-		fault = handle_mm_fault(vma, address, flags, NULL);
+		fault = handle_mm_fault(vma, address, flags, NULL, false);
 
 		if ((fault & VM_FAULT_RETRY) && fatal_signal_pending(current))
 			goto out_nosemaphore;
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 7c5eedd8947a..fcdb770f3912 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -1210,7 +1210,8 @@ NOKPROBE_SYMBOL(do_kern_addr_fault);
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
 			unsigned long error_code,
-			unsigned long address)
+			unsigned long address,
+			bool trace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
@@ -1326,7 +1327,11 @@ void do_user_addr_fault(struct pt_regs *regs,
 	if (!(flags & FAULT_FLAG_USER))
 		goto lock_mmap;
 
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(lock_vma_under_rcu_start);
 	vma = lock_vma_under_rcu(mm, address);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(lock_vma_under_rcu_end);
 	if (!vma)
 		goto lock_mmap;
 
@@ -1335,7 +1340,12 @@ void do_user_addr_fault(struct pt_regs *regs,
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
 		return;
 	}
-	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(handle_mm_fault_with_vma_lock_start);
+	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs, trace);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(handle_mm_fault_with_vma_lock_end);
+
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -1358,7 +1368,20 @@ void do_user_addr_fault(struct pt_regs *regs,
 lock_mmap:
 
 retry:
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_lock_mm_and_find_vma_start);
+		else
+			fast_tracepoint(first_try_lock_mm_and_find_vma_start);
+	}
 	vma = lock_mm_and_find_vma(mm, address, regs);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_lock_mm_and_find_vma_end);
+		else
+			fast_tracepoint(first_try_lock_mm_and_find_vma_end);
+	}
+
 	if (unlikely(!vma)) {
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
@@ -1386,7 +1409,19 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_start);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_start);
+	}
+	fault = handle_mm_fault(vma, address, flags, regs, trace);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_end);
+	}
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
@@ -1464,7 +1499,7 @@ trace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,
 
 static __always_inline void
 handle_page_fault(struct pt_regs *regs, unsigned long error_code,
-			      unsigned long address)
+			      unsigned long address, bool trace)
 {
 	trace_page_fault_entries(regs, error_code, address);
 
@@ -1475,7 +1510,7 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, error_code, address);
 	} else {
-		do_user_addr_fault(regs, error_code, address);
+		do_user_addr_fault(regs, error_code, address, trace);
 		/*
 		 * User address page fault handling might have reenabled
 		 * interrupts. Fixing up all potential exit points of
@@ -1536,7 +1571,7 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	state = irqentry_enter(regs);
 
 	instrumentation_begin();
-	handle_page_fault(regs, error_code, address);
+	handle_page_fault(regs, error_code, address, trace);
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c
index 16e11b6f6f78..12851e400110 100644
--- a/arch/xtensa/mm/fault.c
+++ b/arch/xtensa/mm/fault.c
@@ -156,7 +156,7 @@ void do_page_fault(struct pt_regs *regs)
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	fault = handle_mm_fault(vma, address, flags, regs, false);
 
 	if (fault_signal_pending(fault, regs)) {
 		if (!user_mode(regs))
diff --git a/drivers/iommu/iommu-sva.c b/drivers/iommu/iommu-sva.c
index 1a51cfd82808..cf2201e2644a 100644
--- a/drivers/iommu/iommu-sva.c
+++ b/drivers/iommu/iommu-sva.c
@@ -245,7 +245,7 @@ iommu_sva_handle_mm(struct iommu_fault *fault, struct mm_struct *mm)
 		/* Access fault */
 		goto out_put_mm;
 
-	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL);
+	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL, false);
 	status = ret & VM_FAULT_ERROR ? IOMMU_PAGE_RESP_INVALID :
 		IOMMU_PAGE_RESP_SUCCESS;
 
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 22f4bf956ba1..b8f08fefbcb6 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -32,6 +32,7 @@
 #include <linux/swapops.h>
 #include <linux/miscdevice.h>
 #include <linux/uio.h>
+#include <trace/fast.h>
 
 static int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -360,7 +361,7 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *mm = vma->vm_mm;
@@ -500,7 +501,11 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	release_fault_lock(vmf);
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_start);
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+			fast_tracepoint(wake_up_userfaultfd_end);
 		schedule();
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 0ef2ba0c667a..a9b021d1ce6f 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2399,7 +2399,7 @@ struct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,
 #ifdef CONFIG_MMU
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
 				  unsigned long address, unsigned int flags,
-				  struct pt_regs *regs);
+				  struct pt_regs *regs, bool trace);
 extern int fixup_user_fault(struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 75342022d144..980b1beffc78 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -82,7 +82,7 @@ struct userfaultfd_ctx {
 	struct mm_struct *mm;
 };
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace);
 
 /* A combined operation mode + behavior flags. */
 typedef unsigned int __bitwise uffd_flags_t;
diff --git a/include/trace/fast.h b/include/trace/fast.h
index 84790021fede..9194bf53c3d2 100644
--- a/include/trace/fast.h
+++ b/include/trace/fast.h
@@ -9,8 +9,27 @@
 
 #include <asm/tsc.h>
 
-#define ENUMERATE_FAST_TRACEPOINTS(x) \
-	x(c_entry)		      \
+#define ENUMERATE_FAST_TRACEPOINTS(x)				      \
+	x(c_entry)						      \
+	x(lock_vma_under_rcu_start)				      \
+	x(lock_vma_under_rcu_end)				      \
+	x(handle_mm_fault_with_vma_lock_start)			      \
+	x(handle_mm_fault_with_vma_lock_page_table_walk_end)	      \
+	x(wake_up_userfaultfd_start)				      \
+	x(wake_up_userfaultfd_end)				      \
+	x(handle_mm_fault_with_vma_lock_end)			      \
+	x(first_try_lock_mm_and_find_vma_start)			      \
+	x(first_try_lock_mm_and_find_vma_end)			      \
+	x(first_try_handle_mm_fault_with_mm_lock_start)		      \
+	x(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end) \
+	x(page_allocation_start)				      \
+	x(page_allocation_end)					      \
+	x(first_try_handle_mm_fault_with_mm_lock_end)		      \
+	x(retry_lock_mm_and_find_vma_start)			      \
+	x(retry_handle_mm_fault_with_mm_lock_start)		      \
+	x(retry_handle_mm_fault_with_mm_lock_page_table_walk_end)     \
+	x(retry_handle_mm_fault_with_mm_lock_end)		      \
+	x(retry_lock_mm_and_find_vma_end)			      \
 	x(c_exit)
 
 #define DECLARE_FAST_TRACEPOINT(name) extern u64 fast_tracepoint_##name;
diff --git a/mm/gup.c b/mm/gup.c
index e065a49842a8..32ae81ba904c 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1183,7 +1183,7 @@ static int faultin_page(struct vm_area_struct *vma,
 		VM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
@@ -1647,7 +1647,7 @@ int fixup_user_fault(struct mm_struct *mm,
 	    fatal_signal_pending(current))
 		return -EINTR;
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
diff --git a/mm/hmm.c b/mm/hmm.c
index feac86196a65..a115415c7bc3 100644
--- a/mm/hmm.c
+++ b/mm/hmm.c
@@ -87,7 +87,7 @@ static int hmm_vma_fault(unsigned long addr, unsigned long end,
 	}
 
 	for (; addr < end; addr += PAGE_SIZE)
-		if (handle_mm_fault(vma, addr, fault_flags, NULL) &
+		if (handle_mm_fault(vma, addr, fault_flags, NULL, false) &
 		    VM_FAULT_ERROR)
 			return -EFAULT;
 	return -EBUSY;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index d3e66136e41a..1716ccabbf82 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1246,7 +1246,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			spin_unlock(vmf->ptl);
 			folio_put(folio);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -1354,7 +1354,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, false);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_folio(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 8746ed2fec13..22e49c79821e 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6404,7 +6404,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_fault *vmf,
 	hugetlb_vma_unlock_read(vmf->vma);
 	hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-	return handle_userfault(vmf, reason);
+	return handle_userfault(vmf, reason, false);
 }
 
 /*
@@ -6804,7 +6804,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			}
 			hugetlb_vma_unlock_read(vma);
 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-			return handle_userfault(&vmf, VM_UFFD_WP);
+			return handle_userfault(&vmf, VM_UFFD_WP, false);
 		}
 
 		vmf.orig_pte = huge_pte_clear_uffd_wp(vmf.orig_pte);
diff --git a/mm/ksm.c b/mm/ksm.c
index 8583fb91ef13..e7f517bcad8f 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -647,7 +647,7 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_v
 			return 0;
 		ret = handle_mm_fault(vma, addr,
 				      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,
-				      NULL);
+				      NULL, false);
 	} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop until we no longer find a KSM page because
diff --git a/mm/memory.c b/mm/memory.c
index 8eba595056fe..8ed1cc6662d9 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -78,6 +78,7 @@
 #include <linux/sched/sysctl.h>
 
 #include <trace/events/kmem.h>
+#include <trace/fast.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -95,7 +96,7 @@
 #endif
 
 static vm_fault_t do_fault(struct vm_fault *vmf);
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf);
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, bool trace);
 static bool vmf_pte_changed(struct vm_fault *vmf);
 
 /*
@@ -3544,7 +3545,7 @@ vm_fault_t __vmf_anon_prepare(struct vm_fault *vmf)
  *   held to the old page, as well as updating the rmap.
  * - In any case, unlock the PTL and drop the reference we took to the old page.
  */
-static vm_fault_t wp_page_copy(struct vm_fault *vmf)
+static vm_fault_t wp_page_copy(struct vm_fault *vmf, bool trace)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
 	struct vm_area_struct *vma = vmf->vma;
@@ -3566,7 +3567,11 @@ static vm_fault_t wp_page_copy(struct vm_fault *vmf)
 		goto out;
 
 	pfn_is_zero = is_zero_pfn(pte_pfn(vmf->orig_pte));
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_start);
 	new_folio = folio_prealloc(mm, vma, vmf->address, pfn_is_zero);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_end);
 	if (!new_folio)
 		goto oom;
 
@@ -3935,7 +3940,7 @@ static bool wp_can_reuse_anon_folio(struct folio *folio,
  * but allow concurrent faults), with pte both mapped and locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_wp_page(struct vm_fault *vmf)
+static vm_fault_t do_wp_page(struct vm_fault *vmf, bool trace)
 	__releases(vmf->ptl)
 {
 	const bool unshare = vmf->flags & FAULT_FLAG_UNSHARE;
@@ -3947,7 +3952,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 		if (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {
 			if (!userfaultfd_wp_async(vma)) {
 				pte_unmap_unlock(vmf->pte, vmf->ptl);
-				return handle_userfault(vmf, VM_UFFD_WP);
+				return handle_userfault(vmf, VM_UFFD_WP, trace);
 			}
 
 			/*
@@ -4027,7 +4032,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 	if (folio && folio_test_ksm(folio))
 		count_vm_event(COW_KSM);
 #endif
-	return wp_page_copy(vmf);
+	return wp_page_copy(vmf, trace);
 }
 
 static void unmap_mapping_range_vma(struct vm_area_struct *vma,
@@ -4243,10 +4248,10 @@ static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
 	return 0;
 }
 
-static vm_fault_t do_pte_missing(struct vm_fault *vmf)
+static vm_fault_t do_pte_missing(struct vm_fault *vmf, bool trace)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_anonymous_page(vmf);
+		return do_anonymous_page(vmf, trace);
 	else
 		return do_fault(vmf);
 }
@@ -4264,7 +4269,7 @@ static vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)
 	if (unlikely(!userfaultfd_wp(vmf->vma)))
 		return pte_marker_clear(vmf);
 
-	return do_pte_missing(vmf);
+	return do_pte_missing(vmf, false);
 }
 
 static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
@@ -4895,7 +4900,7 @@ vm_fault_t do_swap_page(struct vm_fault *vmf)
 	}
 
 	if (vmf->flags & FAULT_FLAG_WRITE) {
-		ret |= do_wp_page(vmf);
+		ret |= do_wp_page(vmf, false);
 		if (ret & VM_FAULT_ERROR)
 			ret &= VM_FAULT_ERROR;
 		goto out;
@@ -5039,7 +5044,7 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	unsigned long addr = vmf->address;
@@ -5078,7 +5083,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 		}
 		goto setpte;
 	}
@@ -5088,7 +5093,11 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (ret)
 		return ret;
 	/* Returns NULL on OOM or ERR_PTR(-EAGAIN) if we must retry the fault */
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_start);
 	folio = alloc_anon_folio(vmf);
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace))
+		fast_tracepoint(page_allocation_end);
 	if (IS_ERR(folio))
 		return 0;
 	if (!folio)
@@ -5128,7 +5137,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		folio_put(folio);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 	}
 
 	folio_ref_add(folio, nr_pages - 1);
@@ -5969,7 +5978,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 		    userfaultfd_huge_pmd_wp(vma, vmf->orig_pmd)) {
 			if (userfaultfd_wp_async(vmf->vma))
 				goto split;
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, false);
 		}
 		return do_huge_pmd_wp_page(vmf);
 	}
@@ -6042,7 +6051,7 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, bool trace)
 {
 	pte_t entry;
 
@@ -6085,8 +6094,17 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 		}
 	}
 
+	if (IS_ENABLED(CONFIG_TRACE_PF) && likely(trace)) {
+		if (vmf->flags & FAULT_FLAG_VMA_LOCK)
+			fast_tracepoint(handle_mm_fault_with_vma_lock_page_table_walk_end);
+		else if (vmf->flags & FAULT_FLAG_TRIED)
+			fast_tracepoint(retry_handle_mm_fault_with_mm_lock_page_table_walk_end);
+		else
+			fast_tracepoint(first_try_handle_mm_fault_with_mm_lock_page_table_walk_end);
+	}
+
 	if (!vmf->pte)
-		return do_pte_missing(vmf);
+		return do_pte_missing(vmf, trace);
 
 	if (!pte_present(vmf->orig_pte))
 		return do_swap_page(vmf);
@@ -6102,7 +6120,7 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 	if (vmf->flags & (FAULT_FLAG_WRITE|FAULT_FLAG_UNSHARE)) {
 		if (!pte_write(entry))
-			return do_wp_page(vmf);
+			return do_wp_page(vmf, trace);
 		else if (likely(vmf->flags & FAULT_FLAG_WRITE))
 			entry = pte_mkdirty(entry);
 	}
@@ -6137,7 +6155,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		bool trace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -6229,7 +6248,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, trace);
 }
 
 /**
@@ -6364,7 +6383,7 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-			   unsigned int flags, struct pt_regs *regs)
+			   unsigned int flags, struct pt_regs *regs, bool trace)
 {
 	/* If the fault handler drops the mmap_lock, vma may be freed */
 	struct mm_struct *mm = vma->vm_mm;
@@ -6398,7 +6417,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, trace);
 
 	/*
 	 * Warning: It is no longer safe to dereference vma-> after this point,
diff --git a/mm/shmem.c b/mm/shmem.c
index 0c5fb4ffa03a..024717c48f69 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2457,7 +2457,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	if (folio && vma && userfaultfd_minor(vma)) {
 		if (!xa_is_value(folio))
 			folio_put(folio);
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, false);
 		return 0;
 	}
 
@@ -2506,7 +2506,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, false);
 		return 0;
 	}
 
-- 
2.49.0

