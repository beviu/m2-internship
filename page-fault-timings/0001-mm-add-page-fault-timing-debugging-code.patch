From 919402fa8e6379cec6ae31ba33acbbf148766fec Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Tue, 4 Mar 2025 13:50:40 +0100
Subject: [PATCH] mm: add page fault timing debugging code

The code activates if ebx contains the value 0xb141a52a before a page
fault.

It collect CPU timestamp counters (TSC) at various points during page
fault handling that are accessible through sysctls and and returns the
value of the TSC just before iret is called in eax:edx.
---
 arch/x86/entry/entry_64.S     | 24 ++++++++++++++++++++
 arch/x86/mm/fault.c           | 41 +++++++++++++++++++++++++++++-----
 drivers/iommu/iommu-sva.c     |  2 +-
 fs/userfaultfd.c              |  6 ++++-
 include/linux/mm.h            |  2 +-
 include/linux/userfaultfd_k.h |  2 +-
 include/trace/fast.h          | 42 +++++++++++++++++++++++++++++++++++
 kernel/trace/Kconfig          |  4 ++++
 kernel/trace/Makefile         |  1 +
 kernel/trace/fast.c           | 31 ++++++++++++++++++++++++++
 mm/gup.c                      |  4 ++--
 mm/huge_memory.c              |  4 ++--
 mm/hugetlb.c                  |  4 ++--
 mm/ksm.c                      |  2 +-
 mm/memory.c                   | 37 +++++++++++++++++-------------
 mm/shmem.c                    |  4 ++--
 16 files changed, 176 insertions(+), 34 deletions(-)
 create mode 100644 include/trace/fast.h
 create mode 100644 kernel/trace/fast.c

diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index f52dbe0ad93c..fb283d685e33 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -336,6 +336,19 @@ SYM_CODE_START(\asmsym)
 	.endif
 
 	ENDBR
+
+	.if \vector == X86_TRAP_PF
+		/* Check for a magic number to read the counter. On a recent CPU, a predicted
+		   branch that is not taken has a latency of less than a cycle, and the cmp
+		   instruction has a latency of a single cycle. */
+		cmpl	$0xb141a52a, %ebx
+		jne	1f
+		ALTERNATIVE_2 "rdtsc", \
+			"lfence; rdtsc", X86_FEATURE_LFENCE_RDTSC, \
+			"rdtscp", X86_FEATURE_RDTSCP
+1:
+	.endif
+
 	ASM_CLAC
 	cld
 
@@ -649,6 +662,17 @@ SYM_INNER_LABEL(early_xen_iret_patch, SYM_L_GLOBAL)
 
 SYM_INNER_LABEL(native_irq_return_iret, SYM_L_GLOBAL)
 	ANNOTATE_NOENDBR // exc_double_fault
+
+	/* Check for a magic number to read the counter. On a recent CPU, a predicted
+	   branch that is not taken has a latency of less than a cycle, and the cmp
+	   instruction has a latency of a single cycle. */
+	cmpl	$0xb141a52a, %ebx
+	jne	1f
+	ALTERNATIVE_2 "rdtsc", \
+		"lfence; rdtsc", X86_FEATURE_LFENCE_RDTSC, \
+		"rdtscp", X86_FEATURE_RDTSCP
+1:
+
 	/*
 	 * This may fault.  Non-paranoid faults on return to userspace are
 	 * handled by fixup_bad_iret.  These include #SS, #GP, and #NP.
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 296d294142c8..95defc081f54 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -36,10 +36,13 @@
 #include <asm/irq_stack.h>
 #include <asm/fred.h>
 #include <asm/sev.h>			/* snp_dump_hva_rmpentry()	*/
+#include <asm/msr.h>
 
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
 
+#include <trace/fast.h>
+
 /*
  * Returns 0 if mmiotrace is disabled, or if the fault is not
  * handled by mmiotrace:
@@ -1209,7 +1212,8 @@ NOKPROBE_SYMBOL(do_kern_addr_fault);
 static inline
 void do_user_addr_fault(struct pt_regs *regs,
 			unsigned long error_code,
-			unsigned long address)
+			unsigned long address,
+			bool trace)
 {
 	struct vm_area_struct *vma;
 	struct task_struct *tsk;
@@ -1325,16 +1329,22 @@ void do_user_addr_fault(struct pt_regs *regs,
 	if (!(flags & FAULT_FLAG_USER))
 		goto lock_mmap;
 
+	FAST_TRACEPOINT(lock_vma_under_rcu_start, trace);
 	vma = lock_vma_under_rcu(mm, address);
 	if (!vma)
 		goto lock_mmap;
+	FAST_TRACEPOINT(lock_vma_under_rcu_end, trace);
 
 	if (unlikely(access_error(error_code, vma))) {
 		bad_area_access_error(regs, error_code, address, NULL, vma);
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
 		return;
 	}
-	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs);
+
+	FAST_TRACEPOINT(first_handle_mm_fault_start, trace);
+	fault = handle_mm_fault(vma, address, flags | FAULT_FLAG_VMA_LOCK, regs, trace);
+	FAST_TRACEPOINT(first_handle_mm_fault_end, trace);
+	
 	if (!(fault & (VM_FAULT_RETRY | VM_FAULT_COMPLETED)))
 		vma_end_read(vma);
 
@@ -1354,10 +1364,14 @@ void do_user_addr_fault(struct pt_regs *regs,
 						 ARCH_DEFAULT_PKEY);
 		return;
 	}
+
 lock_mmap:
 
 retry:
+	FAST_TRACEPOINT(lock_mm_and_find_vma_start, trace);
 	vma = lock_mm_and_find_vma(mm, address, regs);
+	FAST_TRACEPOINT(lock_mm_and_find_vma_end, trace);
+
 	if (unlikely(!vma)) {
 		bad_area_nosemaphore(regs, error_code, address);
 		return;
@@ -1385,7 +1399,9 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 * userland). The return to userland is identified whenever
 	 * FAULT_FLAG_USER|FAULT_FLAG_KILLABLE are both set in flags.
 	 */
-	fault = handle_mm_fault(vma, address, flags, regs);
+	FAST_TRACEPOINT(second_handle_mm_fault_start, trace);
+	fault = handle_mm_fault(vma, address, flags, regs, trace);
+	FAST_TRACEPOINT(second_handle_mm_fault_end, trace);
 
 	if (fault_signal_pending(fault, regs)) {
 		/*
@@ -1466,7 +1482,7 @@ trace_page_fault_entries(struct pt_regs *regs, unsigned long error_code,
 
 static __always_inline void
 handle_page_fault(struct pt_regs *regs, unsigned long error_code,
-			      unsigned long address)
+			      unsigned long address, bool trace)
 {
 	trace_page_fault_entries(regs, error_code, address);
 
@@ -1477,7 +1493,7 @@ handle_page_fault(struct pt_regs *regs, unsigned long error_code,
 	if (unlikely(fault_in_kernel_space(address))) {
 		do_kern_addr_fault(regs, error_code, address);
 	} else {
-		do_user_addr_fault(regs, error_code, address);
+		do_user_addr_fault(regs, error_code, address, trace);
 		/*
 		 * User address page fault handling might have reenabled
 		 * interrupts. Fixing up all potential exit points of
@@ -1493,6 +1509,16 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 {
 	irqentry_state_t state;
 	unsigned long address;
+	bool trace = (regs->bx & 0xffffffff) == 0xb141a52a;
+
+	if (likely(trace)) {
+		/* Save the PF ISR entry timestamp because it will be overwritten in the
+		   assembly return code. */
+		regs->di = regs->ax & 0xffffffff;
+		regs->si = regs->dx & 0xffffffff;
+
+		timestamp_c_entry = rdtsc_ordered();
+	}
 
 	address = cpu_feature_enabled(X86_FEATURE_FRED) ? fred_event_data(regs) : read_cr2();
 
@@ -1535,8 +1561,11 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	state = irqentry_enter(regs);
 
 	instrumentation_begin();
-	handle_page_fault(regs, error_code, address);
+	handle_page_fault(regs, error_code, address, trace);
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
+
+	if (likely(trace))
+		timestamp_c_exit = rdtsc_ordered();
 }
diff --git a/drivers/iommu/iommu-sva.c b/drivers/iommu/iommu-sva.c
index 503c5d23c1ea..36e8534694b6 100644
--- a/drivers/iommu/iommu-sva.c
+++ b/drivers/iommu/iommu-sva.c
@@ -248,7 +248,7 @@ iommu_sva_handle_mm(struct iommu_fault *fault, struct mm_struct *mm)
 		/* Access fault */
 		goto out_put_mm;
 
-	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL);
+	ret = handle_mm_fault(vma, prm->addr, fault_flags, NULL, false);
 	status = ret & VM_FAULT_ERROR ? IOMMU_PAGE_RESP_INVALID :
 		IOMMU_PAGE_RESP_SUCCESS;
 
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 97c4d71115d8..5bc9a0762bb6 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -32,6 +32,8 @@
 #include <linux/swapops.h>
 #include <linux/miscdevice.h>
 #include <linux/uio.h>
+#include <asm/msr.h>
+#include <trace/fast.h>
 
 static int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -360,7 +362,7 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *mm = vma->vm_mm;
@@ -501,7 +503,9 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 	release_fault_lock(vmf);
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
+		FAST_TRACEPOINT(wake_up_userfaultfd_start, trace);
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		FAST_TRACEPOINT(wake_up_userfaultfd_end, trace);
 		schedule();
 	}
 
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 7b1068ddcbb7..d2e52676ac03 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2442,7 +2442,7 @@ struct vm_area_struct *lock_mm_and_find_vma(struct mm_struct *mm,
 #ifdef CONFIG_MMU
 extern vm_fault_t handle_mm_fault(struct vm_area_struct *vma,
 				  unsigned long address, unsigned int flags,
-				  struct pt_regs *regs);
+				  struct pt_regs *regs, bool trace);
 extern int fixup_user_fault(struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
 			    bool *unlocked);
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 75342022d144..980b1beffc78 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -82,7 +82,7 @@ struct userfaultfd_ctx {
 	struct mm_struct *mm;
 };
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason, bool trace);
 
 /* A combined operation mode + behavior flags. */
 typedef unsigned int __bitwise uffd_flags_t;
diff --git a/include/trace/fast.h b/include/trace/fast.h
new file mode 100644
index 000000000000..3873852d3770
--- /dev/null
+++ b/include/trace/fast.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef _TRACE_FAST_H
+#define _TRACE_FAST_H
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+
+#include <asm/msr.h>
+
+#define ENUMERATE_FAST_TRACEPOINTS(x) \
+	x(c_entry) \
+	x(lock_vma_under_rcu_start) \
+	x(lock_vma_under_rcu_end) \
+	x(first_handle_mm_fault_start) \
+	x(first_page_table_walk_end) \
+	x(wake_up_userfaultfd_start) \
+	x(wake_up_userfaultfd_end) \
+	x(first_handle_mm_fault_end) \
+	x(lock_mm_and_find_vma_start) \
+	x(lock_mm_and_find_vma_end) \
+	x(second_handle_mm_fault_start) \
+	x(second_page_table_walk_end) \
+	x(second_handle_mm_fault_end) \
+	x(c_exit)
+
+#ifdef CONFIG_FAST_TRACEPOINTS
+
+#define DECLARE_TIMESTAMP(name) extern u64 timestamp_##name;
+ENUMERATE_FAST_TRACEPOINTS(DECLARE_TIMESTAMP)
+#undef DECLARE_TIMESTAMP
+
+#define FAST_TRACEPOINT(name, trace) \
+	if (likely(trace)) \
+		timestamp_##name = rdtsc_ordered()
+
+#else
+
+#define FAST_TRACEPOINT(name, trace)
+		
+#endif /* CONFIG_FAST_TRACEPOINTS */
+
+#endif /* _TRACE_FAST_H */
diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig
index d570b8b9c0a9..b2cb79065637 100644
--- a/kernel/trace/Kconfig
+++ b/kernel/trace/Kconfig
@@ -533,6 +533,10 @@ config MMIOTRACE
 	  See Documentation/trace/mmiotrace.rst.
 	  If you are not helping to develop drivers, say N.
 
+config FAST_TRACEPOINTS
+	bool "Fast tracepoints"
+	default n
+
 config ENABLE_DEFAULT_TRACERS
 	bool "Trace process context switches and events"
 	depends on !GENERIC_TRACER
diff --git a/kernel/trace/Makefile b/kernel/trace/Makefile
index 057cd975d014..92cca87e1f70 100644
--- a/kernel/trace/Makefile
+++ b/kernel/trace/Makefile
@@ -106,6 +106,7 @@ obj-$(CONFIG_FTRACE_RECORD_RECURSION) += trace_recursion_record.o
 obj-$(CONFIG_FPROBE) += fprobe.o
 obj-$(CONFIG_RETHOOK) += rethook.o
 obj-$(CONFIG_FPROBE_EVENTS) += trace_fprobe.o
+obj-$(CONFIG_FAST_TRACEPOINTS) += fast.o
 
 obj-$(CONFIG_TRACEPOINT_BENCHMARK) += trace_benchmark.o
 obj-$(CONFIG_RV) += rv/
diff --git a/kernel/trace/fast.c b/kernel/trace/fast.c
new file mode 100644
index 000000000000..e4e0a6b39f86
--- /dev/null
+++ b/kernel/trace/fast.c
@@ -0,0 +1,31 @@
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/sysctl.h>
+
+#include <trace/fast.h>
+
+#define DEFINE_TRACEPOINT(name) u64 timestamp_##name;
+ENUMERATE_FAST_TRACEPOINTS(DEFINE_TRACEPOINT)
+#undef DEFINE_TRACEPOINT
+
+static const struct ctl_table timestamp_ctls[] = {
+#define DEFINE_CTL(name)                                \
+	{                                               \
+		.procname = #name,                      \
+		.data = &timestamp_##name,              \
+		.maxlen = sizeof(timestamp_##name),     \
+		.mode = 0444,                           \
+		.proc_handler = proc_doulongvec_minmax, \
+	},
+	ENUMERATE_FAST_TRACEPOINTS(DEFINE_CTL)
+#undef DEFINE_CTL
+};
+
+static int __init fast_tracepoints_init(void)
+{
+	register_sysctl("debug/fast-tracepoints", timestamp_ctls);
+
+	return 0;
+}
+
+early_initcall(fast_tracepoints_init)
diff --git a/mm/gup.c b/mm/gup.c
index 3883b307780e..c34d36854173 100644
--- a/mm/gup.c
+++ b/mm/gup.c
@@ -1193,7 +1193,7 @@ static int faultin_page(struct vm_area_struct *vma,
 		VM_BUG_ON(fault_flags & FAULT_FLAG_WRITE);
 	}
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
@@ -1650,7 +1650,7 @@ int fixup_user_fault(struct mm_struct *mm,
 	    fatal_signal_pending(current))
 		return -EINTR;
 
-	ret = handle_mm_fault(vma, address, fault_flags, NULL);
+	ret = handle_mm_fault(vma, address, fault_flags, NULL, false);
 
 	if (ret & VM_FAULT_COMPLETED) {
 		/*
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 3d3ebdc002d5..cf357090c7b4 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1246,7 +1246,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			spin_unlock(vmf->ptl);
 			folio_put(folio);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -1357,7 +1357,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_folio(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 163190e89ea1..93415a80d2c0 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6071,7 +6071,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_fault *vmf,
 	hugetlb_vma_unlock_read(vmf->vma);
 	hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-	return handle_userfault(vmf, reason);
+	return handle_userfault(vmf, reason, NULL);
 }
 
 /*
@@ -6471,7 +6471,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			}
 			hugetlb_vma_unlock_read(vma);
 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-			return handle_userfault(&vmf, VM_UFFD_WP);
+			return handle_userfault(&vmf, VM_UFFD_WP, NULL);
 		}
 
 		vmf.orig_pte = huge_pte_clear_uffd_wp(vmf.orig_pte);
diff --git a/mm/ksm.c b/mm/ksm.c
index 8be2b144fefd..5c299083bdea 100644
--- a/mm/ksm.c
+++ b/mm/ksm.c
@@ -647,7 +647,7 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr, bool lock_v
 			return 0;
 		ret = handle_mm_fault(vma, addr,
 				      FAULT_FLAG_UNSHARE | FAULT_FLAG_REMOTE,
-				      NULL);
+				      NULL, false);
 	} while (!(ret & (VM_FAULT_SIGBUS | VM_FAULT_SIGSEGV | VM_FAULT_OOM)));
 	/*
 	 * We must loop until we no longer find a KSM page because
diff --git a/mm/memory.c b/mm/memory.c
index b4d3d4893267..57c2b4a3f63c 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -79,6 +79,7 @@
 #include <linux/fsnotify.h>
 
 #include <trace/events/kmem.h>
+#include <trace/fast.h>
 
 #include <asm/io.h>
 #include <asm/mmu_context.h>
@@ -86,6 +87,7 @@
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
+#include <asm/msr.h>
 
 #include "pgalloc-track.h"
 #include "internal.h"
@@ -104,7 +106,7 @@ EXPORT_SYMBOL(mem_map);
 #endif
 
 static vm_fault_t do_fault(struct vm_fault *vmf);
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf);
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs, bool trace);
 static bool vmf_pte_changed(struct vm_fault *vmf);
 
 /*
@@ -3757,7 +3759,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 		if (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {
 			if (!userfaultfd_wp_async(vma)) {
 				pte_unmap_unlock(vmf->pte, vmf->ptl);
-				return handle_userfault(vmf, VM_UFFD_WP);
+				return handle_userfault(vmf, VM_UFFD_WP, NULL);
 			}
 
 			/*
@@ -4050,10 +4052,10 @@ static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
 	return 0;
 }
 
-static vm_fault_t do_pte_missing(struct vm_fault *vmf)
+static vm_fault_t do_pte_missing(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_anonymous_page(vmf);
+		return do_anonymous_page(vmf, regs, trace);
 	else
 		return do_fault(vmf);
 }
@@ -4071,7 +4073,7 @@ static vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)
 	if (unlikely(!userfaultfd_wp(vmf->vma)))
 		return pte_marker_clear(vmf);
 
-	return do_pte_missing(vmf);
+	return do_pte_missing(vmf, NULL, false);
 }
 
 static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
@@ -4838,7 +4840,7 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	unsigned long addr = vmf->address;
@@ -4858,6 +4860,8 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (pte_alloc(vma->vm_mm, vmf->pmd))
 		return VM_FAULT_OOM;
 
+	FAST_TRACEPOINT(first_page_table_walk_end, trace);
+
 	/* Use the zero-page for reads */
 	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
 			!mm_forbids_zeropage(vma->vm_mm)) {
@@ -4877,7 +4881,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, trace);
 		}
 		goto setpte;
 	}
@@ -4927,7 +4931,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		folio_put(folio);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 	}
 
 	folio_ref_add(folio, nr_pages - 1);
@@ -5771,7 +5775,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 		    userfaultfd_huge_pmd_wp(vma, vmf->orig_pmd)) {
 			if (userfaultfd_wp_async(vmf->vma))
 				goto split;
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, NULL);
 		}
 		return do_huge_pmd_wp_page(vmf);
 	}
@@ -5853,7 +5857,7 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, struct pt_regs *regs, bool trace)
 {
 	pte_t entry;
 
@@ -5897,7 +5901,9 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 
 	if (!vmf->pte)
-		return do_pte_missing(vmf);
+		return do_pte_missing(vmf, regs, trace);
+
+	FAST_TRACEPOINT(second_page_table_walk_end, trace);
 
 	if (!pte_present(vmf->orig_pte))
 		return do_swap_page(vmf);
@@ -5948,7 +5954,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		struct pt_regs *regs, bool trace)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -6040,7 +6047,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, regs, trace);
 }
 
 /**
@@ -6175,7 +6182,7 @@ static vm_fault_t sanitize_fault_flags(struct vm_area_struct *vma,
  * return value.  See filemap_fault() and __folio_lock_or_retry().
  */
 vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
-			   unsigned int flags, struct pt_regs *regs)
+			   unsigned int flags, struct pt_regs *regs, bool trace)
 {
 	/* If the fault handler drops the mmap_lock, vma may be freed */
 	struct mm_struct *mm = vma->vm_mm;
@@ -6209,7 +6216,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, regs, trace);
 
 	/*
 	 * Warning: It is no longer safe to dereference vma-> after this point,
diff --git a/mm/shmem.c b/mm/shmem.c
index 4ea6109a8043..8a482d94c476 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2447,7 +2447,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	if (folio && vma && userfaultfd_minor(vma)) {
 		if (!xa_is_value(folio))
 			folio_put(folio);
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, NULL);
 		return 0;
 	}
 
@@ -2496,7 +2496,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 		return 0;
 	}
 
-- 
2.48.1

