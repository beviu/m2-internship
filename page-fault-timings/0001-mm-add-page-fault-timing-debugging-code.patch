From a4c00da680af05f0d7959d8e7c90b4f2d724caa0 Mon Sep 17 00:00:00 2001
From: beviu <contact@beviu.com>
Date: Tue, 4 Mar 2025 13:50:40 +0100
Subject: [PATCH] mm: add page fault timing debugging code

The code activates if ebx contains the value 0xb141a52a before a page
fault.

It prints CPU timestamp counters (TSC) at various points during page
fault handling to dmesg and returns:
- the value of the TSC just before iret is called in eax:edx, and
- the value of the TSC before registers are restored in ebx:ecx.
---
 arch/x86/Kconfig.debug        |  4 ++++
 arch/x86/entry/entry_64.S     | 19 +++++++++++++++++++
 arch/x86/include/asm/msr.h    | 31 +++++++++++++++++++++++++++++++
 arch/x86/mm/fault.c           | 15 +++++++++++++++
 fs/userfaultfd.c              |  5 ++++-
 include/linux/userfaultfd_k.h |  3 ++-
 mm/huge_memory.c              |  4 ++--
 mm/hugetlb.c                  |  4 ++--
 mm/memory.c                   | 34 ++++++++++++++++++++--------------
 mm/shmem.c                    |  4 ++--
 10 files changed, 101 insertions(+), 22 deletions(-)

diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug
index 1eb4d23cdaae..af9b03a1d458 100644
--- a/arch/x86/Kconfig.debug
+++ b/arch/x86/Kconfig.debug
@@ -107,6 +107,10 @@ config IOMMU_LEAK
 	  Add a simple leak tracer to the IOMMU code. This is useful when you
 	  are debugging a buggy device driver that leaks IOMMU mappings.
 
+config TIMINGS_WITH_RDTSC_ORDERED
+	bool "Use rdtsc_ordered to measure timings"
+	default y
+
 config HAVE_MMIOTRACE_SUPPORT
 	def_bool y
 
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index f52dbe0ad93c..9b86cbfe099d 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -336,6 +336,16 @@ SYM_CODE_START(\asmsym)
 	.endif
 
 	ENDBR
+
+	.if \vector == X86_TRAP_PF
+		/* Check for a magic number to read the counter. */
+		cmpl	$0xb141a52a, %ebx
+		jne	1f
+		rdtsc
+		lfence
+1:
+	.endif
+
 	ASM_CLAC
 	cld
 
@@ -649,6 +659,15 @@ SYM_INNER_LABEL(early_xen_iret_patch, SYM_L_GLOBAL)
 
 SYM_INNER_LABEL(native_irq_return_iret, SYM_L_GLOBAL)
 	ANNOTATE_NOENDBR // exc_double_fault
+
+	/* Check for a magic number to read the counter. */
+	cmpl	$0xb141a52a, %ebx
+	jne	1f
+	movl	%eax, %ebx
+	movl	%edx, %ecx
+	rdtsc
+1:
+
 	/*
 	 * This may fault.  Non-paranoid faults on return to userspace are
 	 * handled by fixup_bad_iret.  These include #SS, #GP, and #NP.
diff --git a/arch/x86/include/asm/msr.h b/arch/x86/include/asm/msr.h
index 001853541f1e..6120eb4e9047 100644
--- a/arch/x86/include/asm/msr.h
+++ b/arch/x86/include/asm/msr.h
@@ -224,6 +224,37 @@ static __always_inline unsigned long long rdtsc_ordered(void)
 	return EAX_EDX_VAL(val, low, high);
 }
 
+static __always_inline void end_timing_section(struct pt_regs *regs, const char *name)
+{
+	unsigned long long section_end;
+	unsigned long long section_start;
+	unsigned long long next_section_start;
+
+#ifdef CONFIG_TIMINGS_WITH_RDTSC_ORDERED
+	section_end = rdtsc_ordered();
+#else
+	section_end = rdtsc();
+#endif
+	
+	if ((regs->bx & 0xffffffff) != 0xb141a52a || !regs)
+		return;
+
+	section_start = (unsigned long long)regs->ax |
+		((unsigned long long)regs->dx << 32);
+
+	/* TODO: Stop using printk because it impacts the cache! */
+	printk("%s: start=%llu, end=%llu\n", name, section_start, section_end);
+
+#ifdef CONFIG_TIMINGS_WITH_RDTSC_ORDERED
+	next_section_start = rdtsc_ordered();
+#else
+	next_section_start = rdtsc();
+#endif
+
+	regs->ax = next_section_start & 0xffffffff;
+	regs->dx = next_section_start >> 32;
+}
+
 static inline unsigned long long native_read_pmc(int counter)
 {
 	DECLARE_ARGS(val, low, high);
diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c
index 296d294142c8..bad1872ae8c1 100644
--- a/arch/x86/mm/fault.c
+++ b/arch/x86/mm/fault.c
@@ -36,6 +36,7 @@
 #include <asm/irq_stack.h>
 #include <asm/fred.h>
 #include <asm/sev.h>			/* snp_dump_hva_rmpentry()	*/
+#include <asm/msr.h>
 
 #define CREATE_TRACE_POINTS
 #include <asm/trace/exceptions.h>
@@ -1329,6 +1330,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 	if (!vma)
 		goto lock_mmap;
 
+	end_timing_section(regs, "read_lock_vma");
+
 	if (unlikely(access_error(error_code, vma))) {
 		bad_area_access_error(regs, error_code, address, NULL, vma);
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
@@ -1340,6 +1343,7 @@ void do_user_addr_fault(struct pt_regs *regs,
 
 	if (!(fault & VM_FAULT_RETRY)) {
 		count_vm_vma_lock_event(VMA_LOCK_SUCCESS);
+		end_timing_section(regs, "handle_mm_fault");
 		goto done;
 	}
 	count_vm_vma_lock_event(VMA_LOCK_RETRY);
@@ -1354,6 +1358,9 @@ void do_user_addr_fault(struct pt_regs *regs,
 						 ARCH_DEFAULT_PKEY);
 		return;
 	}
+
+	end_timing_section(regs, "handle_mm_fault");
+
 lock_mmap:
 
 retry:
@@ -1363,6 +1370,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 		return;
 	}
 
+	end_timing_section(regs, "retry_read_lock_vma");
+
 	/*
 	 * Ok, we have a good vm_area for this memory access, so
 	 * we can handle it..
@@ -1387,6 +1396,8 @@ void do_user_addr_fault(struct pt_regs *regs,
 	 */
 	fault = handle_mm_fault(vma, address, flags, regs);
 
+	end_timing_section(regs, "retry_handle_mm_fault");
+
 	if (fault_signal_pending(fault, regs)) {
 		/*
 		 * Quick path to respond to signals.  The core mm code
@@ -1494,6 +1505,8 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	irqentry_state_t state;
 	unsigned long address;
 
+	end_timing_section(regs, "save_state");
+
 	address = cpu_feature_enabled(X86_FEATURE_FRED) ? fred_event_data(regs) : read_cr2();
 
 	prefetchw(&current->mm->mmap_lock);
@@ -1539,4 +1552,6 @@ DEFINE_IDTENTRY_RAW_ERRORCODE(exc_page_fault)
 	instrumentation_end();
 
 	irqentry_exit(regs, state);
+
+	end_timing_section(regs, "cleanup");
 }
diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c
index 97c4d71115d8..23297e9a8362 100644
--- a/fs/userfaultfd.c
+++ b/fs/userfaultfd.c
@@ -32,6 +32,7 @@
 #include <linux/swapops.h>
 #include <linux/miscdevice.h>
 #include <linux/uio.h>
+#include <asm/msr.h>
 
 static int sysctl_unprivileged_userfaultfd __read_mostly;
 
@@ -360,7 +361,8 @@ static inline unsigned int userfaultfd_get_blocking_state(unsigned int flags)
  * fatal_signal_pending()s, and the mmap_lock must be released before
  * returning it.
  */
-vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
+vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason,
+			    struct pt_regs *regs)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct mm_struct *mm = vma->vm_mm;
@@ -502,6 +504,7 @@ vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason)
 
 	if (likely(must_wait && !READ_ONCE(ctx->released))) {
 		wake_up_poll(&ctx->fd_wqh, EPOLLIN);
+		end_timing_section(regs, "wake_up_userfaultfd");
 		schedule();
 	}
 
diff --git a/include/linux/userfaultfd_k.h b/include/linux/userfaultfd_k.h
index 75342022d144..5dc89f5535b3 100644
--- a/include/linux/userfaultfd_k.h
+++ b/include/linux/userfaultfd_k.h
@@ -82,7 +82,8 @@ struct userfaultfd_ctx {
 	struct mm_struct *mm;
 };
 
-extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason);
+extern vm_fault_t handle_userfault(struct vm_fault *vmf, unsigned long reason,
+				   struct pt_regs *regs);
 
 /* A combined operation mode + behavior flags. */
 typedef unsigned int __bitwise uffd_flags_t;
diff --git a/mm/huge_memory.c b/mm/huge_memory.c
index 3d3ebdc002d5..cf357090c7b4 100644
--- a/mm/huge_memory.c
+++ b/mm/huge_memory.c
@@ -1246,7 +1246,7 @@ static vm_fault_t __do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			spin_unlock(vmf->ptl);
 			folio_put(folio);
 			pte_free(vma->vm_mm, pgtable);
-			ret = handle_userfault(vmf, VM_UFFD_MISSING);
+			ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 			VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			return ret;
 		}
@@ -1357,7 +1357,7 @@ vm_fault_t do_huge_pmd_anonymous_page(struct vm_fault *vmf)
 			} else if (userfaultfd_missing(vma)) {
 				spin_unlock(vmf->ptl);
 				pte_free(vma->vm_mm, pgtable);
-				ret = handle_userfault(vmf, VM_UFFD_MISSING);
+				ret = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 				VM_BUG_ON(ret & VM_FAULT_FALLBACK);
 			} else {
 				set_huge_zero_folio(pgtable, vma->vm_mm, vma,
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 163190e89ea1..93415a80d2c0 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -6071,7 +6071,7 @@ static inline vm_fault_t hugetlb_handle_userfault(struct vm_fault *vmf,
 	hugetlb_vma_unlock_read(vmf->vma);
 	hash = hugetlb_fault_mutex_hash(mapping, vmf->pgoff);
 	mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-	return handle_userfault(vmf, reason);
+	return handle_userfault(vmf, reason, NULL);
 }
 
 /*
@@ -6471,7 +6471,7 @@ vm_fault_t hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,
 			}
 			hugetlb_vma_unlock_read(vma);
 			mutex_unlock(&hugetlb_fault_mutex_table[hash]);
-			return handle_userfault(&vmf, VM_UFFD_WP);
+			return handle_userfault(&vmf, VM_UFFD_WP, NULL);
 		}
 
 		vmf.orig_pte = huge_pte_clear_uffd_wp(vmf.orig_pte);
diff --git a/mm/memory.c b/mm/memory.c
index b4d3d4893267..0dc8ec5195ef 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -86,6 +86,7 @@
 #include <linux/uaccess.h>
 #include <asm/tlb.h>
 #include <asm/tlbflush.h>
+#include <asm/msr.h>
 
 #include "pgalloc-track.h"
 #include "internal.h"
@@ -104,7 +105,7 @@ EXPORT_SYMBOL(mem_map);
 #endif
 
 static vm_fault_t do_fault(struct vm_fault *vmf);
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf);
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs);
 static bool vmf_pte_changed(struct vm_fault *vmf);
 
 /*
@@ -3757,7 +3758,7 @@ static vm_fault_t do_wp_page(struct vm_fault *vmf)
 		if (userfaultfd_pte_wp(vma, ptep_get(vmf->pte))) {
 			if (!userfaultfd_wp_async(vma)) {
 				pte_unmap_unlock(vmf->pte, vmf->ptl);
-				return handle_userfault(vmf, VM_UFFD_WP);
+				return handle_userfault(vmf, VM_UFFD_WP, NULL);
 			}
 
 			/*
@@ -4050,10 +4051,10 @@ static vm_fault_t pte_marker_clear(struct vm_fault *vmf)
 	return 0;
 }
 
-static vm_fault_t do_pte_missing(struct vm_fault *vmf)
+static vm_fault_t do_pte_missing(struct vm_fault *vmf, struct pt_regs *regs)
 {
 	if (vma_is_anonymous(vmf->vma))
-		return do_anonymous_page(vmf);
+		return do_anonymous_page(vmf, regs);
 	else
 		return do_fault(vmf);
 }
@@ -4071,7 +4072,7 @@ static vm_fault_t pte_marker_handle_uffd_wp(struct vm_fault *vmf)
 	if (unlikely(!userfaultfd_wp(vmf->vma)))
 		return pte_marker_clear(vmf);
 
-	return do_pte_missing(vmf);
+	return do_pte_missing(vmf, NULL);
 }
 
 static vm_fault_t handle_pte_marker(struct vm_fault *vmf)
@@ -4838,7 +4839,7 @@ static struct folio *alloc_anon_folio(struct vm_fault *vmf)
  * but allow concurrent faults), and pte mapped but not yet locked.
  * We return with mmap_lock still held, but pte unmapped and unlocked.
  */
-static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
+static vm_fault_t do_anonymous_page(struct vm_fault *vmf, struct pt_regs *regs)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	unsigned long addr = vmf->address;
@@ -4858,6 +4859,8 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (pte_alloc(vma->vm_mm, vmf->pmd))
 		return VM_FAULT_OOM;
 
+	end_timing_section(regs, "walk_page_table");
+
 	/* Use the zero-page for reads */
 	if (!(vmf->flags & FAULT_FLAG_WRITE) &&
 			!mm_forbids_zeropage(vma->vm_mm)) {
@@ -4877,7 +4880,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 		/* Deliver the page fault to userland, check inside PT lock */
 		if (userfaultfd_missing(vma)) {
 			pte_unmap_unlock(vmf->pte, vmf->ptl);
-			return handle_userfault(vmf, VM_UFFD_MISSING);
+			return handle_userfault(vmf, VM_UFFD_MISSING, regs);
 		}
 		goto setpte;
 	}
@@ -4927,7 +4930,7 @@ static vm_fault_t do_anonymous_page(struct vm_fault *vmf)
 	if (userfaultfd_missing(vma)) {
 		pte_unmap_unlock(vmf->pte, vmf->ptl);
 		folio_put(folio);
-		return handle_userfault(vmf, VM_UFFD_MISSING);
+		return handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 	}
 
 	folio_ref_add(folio, nr_pages - 1);
@@ -5771,7 +5774,7 @@ static inline vm_fault_t wp_huge_pmd(struct vm_fault *vmf)
 		    userfaultfd_huge_pmd_wp(vma, vmf->orig_pmd)) {
 			if (userfaultfd_wp_async(vmf->vma))
 				goto split;
-			return handle_userfault(vmf, VM_UFFD_WP);
+			return handle_userfault(vmf, VM_UFFD_WP, NULL);
 		}
 		return do_huge_pmd_wp_page(vmf);
 	}
@@ -5853,7 +5856,7 @@ static vm_fault_t wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)
  * The mmap_lock may have been released depending on flags and our return value.
  * See filemap_fault() and __folio_lock_or_retry().
  */
-static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
+static vm_fault_t handle_pte_fault(struct vm_fault *vmf, struct pt_regs *regs)
 {
 	pte_t entry;
 
@@ -5897,7 +5900,9 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
 	}
 
 	if (!vmf->pte)
-		return do_pte_missing(vmf);
+		return do_pte_missing(vmf, regs);
+
+	end_timing_section(regs, "retry_walk_page_table");
 
 	if (!pte_present(vmf->orig_pte))
 		return do_swap_page(vmf);
@@ -5948,7 +5953,8 @@ static vm_fault_t handle_pte_fault(struct vm_fault *vmf)
  * and __folio_lock_or_retry().
  */
 static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
-		unsigned long address, unsigned int flags)
+		unsigned long address, unsigned int flags,
+		struct pt_regs *regs)
 {
 	struct vm_fault vmf = {
 		.vma = vma,
@@ -6040,7 +6046,7 @@ static vm_fault_t __handle_mm_fault(struct vm_area_struct *vma,
 		}
 	}
 
-	return handle_pte_fault(&vmf);
+	return handle_pte_fault(&vmf, regs);
 }
 
 /**
@@ -6209,7 +6215,7 @@ vm_fault_t handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma->vm_mm, vma, address, flags);
 	else
-		ret = __handle_mm_fault(vma, address, flags);
+		ret = __handle_mm_fault(vma, address, flags, regs);
 
 	/*
 	 * Warning: It is no longer safe to dereference vma-> after this point,
diff --git a/mm/shmem.c b/mm/shmem.c
index 4ea6109a8043..8a482d94c476 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -2447,7 +2447,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	if (folio && vma && userfaultfd_minor(vma)) {
 		if (!xa_is_value(folio))
 			folio_put(folio);
-		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MINOR, NULL);
 		return 0;
 	}
 
@@ -2496,7 +2496,7 @@ static int shmem_get_folio_gfp(struct inode *inode, pgoff_t index,
 	 */
 
 	if (vma && userfaultfd_missing(vma)) {
-		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING);
+		*fault_type = handle_userfault(vmf, VM_UFFD_MISSING, NULL);
 		return 0;
 	}
 
-- 
2.48.1

